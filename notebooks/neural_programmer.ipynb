{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Neural Programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "import autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, Image, clear_output, display\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sys.path.append('../neural_programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "import data_utils\n",
    "from neural_programmer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths, parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one GPU on the multi-GPU machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# WikiTableQuestions data\n",
    "DATA_DIR = '../wtq_data'\n",
    "PERTURBED_DATA_DIR = '../perturbed_wtq_data'\n",
    "\n",
    "# Pretrained model\n",
    "MODEL_FILE = os.path.join('..', 'pretrained_model', 'model_92500')\n",
    "\n",
    "# Output directory to write attributions\n",
    "OUT_DIR = '/scratch/pramodkm/acl18/neural_programmer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, build graph and restore pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Annotated examples loaded ', 14152)\n",
      "('Annotated examples loaded ', 4344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-322e4283c5b9>\", line 1, in <module>\n",
      "    train_data, dev_data, test_data, utility = notebook_utils.init_data(DATA_DIR)\n",
      "  File \"../neural_programmer/notebook_utils.py\", line 33, in init_data\n",
      "    train_data, dev_data, test_data = dat.load()\n",
      "  File \"../neural_programmer/wiki_data.py\", line 529, in load\n",
      "    self.load_annotated_tables()\n",
      "  File \"../neural_programmer/wiki_data.py\", line 359, in load_annotated_tables\n",
      "    entry = self.pre_process_sentence(tokens, ner_tags, ner_values)\n",
      "  File \"../neural_programmer/wiki_data.py\", line 272, in pre_process_sentence\n",
      "    if (tokens[i] == \"score\"):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 736, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 705, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, utility = notebook_utils.init_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess, graph, params = notebook_utils.build_graph(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess, graph = notebook_utils.restore_model(sess, graph, params, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.37195600942655144)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-90cd89ffa83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m92500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "num_correct, num_examples, correct_dict = evaluate(sess, dev_data, utility.FLAGS.batch_size, graph, 92500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0320393476b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_correct\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2831.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'num_correct' is not defined"
     ]
    }
   ],
   "source": [
    "num_correct/2831.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients (IG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = graph.batch_size\n",
    "\n",
    "for offset in range(0, len(data) - graph.batch_size + 1, graph.batch_size):\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, graph.batch_size, graph)\n",
    "\n",
    "    # first run inference to get operator and column sequences, and embeddings of question words\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.question_words_embeddings]\n",
    "    correct_list, operation_softmax, column_softmax, question_words_embeddings = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute table-specific default programs for tables in this batch\n",
    "    feed_copy = feed_dict.copy()\n",
    "    for t in graph.question_words_embeddings:\n",
    "        feed_copy[t] = np.concatenate(\n",
    "            [np.expand_dims(dummy_embedding, 0)]*batch_size, 0)\n",
    "\n",
    "    # Ideally the following line should be uncommented, but for attributions,\n",
    "    # we choose to keep this variable fixed. Note that this induces some bias\n",
    "    # in the attributions as the baseline is no longer an \"empty\" question, but\n",
    "    # an empty question where the question length is implicitly encoded in this variable\n",
    "    # feed_copy[graph.batch_question_attention_mask].fill(question_attention_mask_value)\n",
    "\n",
    "    feed_copy[graph.batch_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_exact_match])\n",
    "    feed_copy[graph.batch_column_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_column_exact_match])\n",
    "\n",
    "    fetches = [graph.final_operation_softmax, graph.final_column_softmax]\n",
    "\n",
    "    default_operation_softmax, default_column_softmax = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset+batch_id]\n",
    "\n",
    "        # get operator indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        default_op_list = notebook_utils.softmax_to_names(\n",
    "            default_operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        default_col_list = notebook_utils.softmax_to_names(\n",
    "            default_column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # Sample points along the integral path and collect them as one batch\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:  # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0/num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "\n",
    "        exact_match = wiki_example.exact_match\n",
    "        exact_column_match = wiki_example.exact_column_match\n",
    "\n",
    "        batch_question_embeddings = np.array(question_words_embeddings)[\n",
    "            :, batch_id, :]  # shape: 62 x 256\n",
    "\n",
    "        # split up set of points into batch_size'd batches\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            # scale question words to points between dummy_embedding and actual embedding\n",
    "            qw_jump = [None]*graph.question_length\n",
    "            for i, t in enumerate(graph.question_words_embeddings):\n",
    "                qw_jump[i] = scale * \\\n",
    "                    (batch_question_embeddings[i] - dummy_embedding)\n",
    "                scaled_feed[t] = [dummy_embedding + j*qw_jump[i]\n",
    "                                  for j in range(k, k+batch_size)]\n",
    "\n",
    "            # scale batch_exact_match\n",
    "            scaled_exact_match = []\n",
    "            scaled_column_exact_match = []\n",
    "\n",
    "            exact_match_jump = [None]*(graph.num_cols + graph.num_word_cols)\n",
    "            exact_column_match_jump = [None] * \\\n",
    "                (graph.num_cols + graph.num_word_cols)\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy columns\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = scale*np.array(exact_match[i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = scale * \\\n",
    "                        np.array(exact_column_match[i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[graph.num_cols+i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_match[graph.num_cols+i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[graph.num_cols + i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_column_match[graph.num_cols + i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[graph.num_cols+i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[graph.num_cols + i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = 0\n",
    "\n",
    "            scaled_feed[graph.batch_exact_match] = np.concatenate(\n",
    "                scaled_exact_match, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.batch_column_exact_match] = np.concatenate(\n",
    "                scaled_column_exact_match, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax, graph.operator_gradients,\n",
    "                       graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([operator_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([operator_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([column_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([column_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check to make sure the integral summation adds up to function difference\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "        question_begin = np.nonzero(\n",
    "            wiki_example.question_attention_mask)[0].shape[0]\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [graph.question_length - question_begin + 2, 2 * graph.max_passes])\n",
    "        row_labels = []  # question words, tm, cm\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for ix in range(question_begin, graph.question_length):\n",
    "            word = utility.reverse_word_ids[wiki_example.question[ix]]\n",
    "            if word == utility.unk_token:\n",
    "                word = word + '-' + [str(w) for w in wiki_example.string_question if w !=\n",
    "                                     wiki_example.question_number and w != wiki_example.question_number_1][ix - question_begin]\n",
    "            word = notebook_utils.rename(word)\n",
    "            row_labels.append(word)\n",
    "        row_labels.extend(['tm', 'cm'])\n",
    "\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, question_begin:]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, question_begin:]\n",
    "\n",
    "        question_string = ' '.join([notebook_utils.rename(str(w))\n",
    "                                    for w in wiki_example.string_question])\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write(question_string)\n",
    "            outf.write('\\n')\n",
    "            outf.write(str(correct_list[batch_id] == 1.0))\n",
    "            outf.write('\\n')\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'), attributions_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HTML with visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients on table-specific default programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this file\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions_default_programs')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all unique tables\n",
    "unique_tables = {}\n",
    "for wiki_example in data:\n",
    "    if not wiki_example.table_key in unique_tables:\n",
    "        wiki_example.exact_column_match = np.zeros_like(\n",
    "            wiki_example.exact_column_match).tolist()\n",
    "        wiki_example.exact_match = np.zeros_like(\n",
    "            wiki_example.exact_match).tolist()\n",
    "        wiki_example.question = [\n",
    "            utility.dummy_token_id] * graph.question_length\n",
    "        wiki_example.question_attention_mask = (question_attention_mask_value * \\\n",
    "            np.ones_like(wiki_example.question_attention_mask)).tolist()\n",
    "        unique_tables[wiki_example.table_key] = wiki_example\n",
    "data = list(unique_tables.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(0, len(data) - graph.batch_size + 1, batch_size):\n",
    "\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, batch_size, graph)\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.column_hidden_vectors, graph.word_column_hidden_vectors]\n",
    "    correct_list, operation_softmax, column_softmax, column_hidden_vectors, word_column_hidden_vectors = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute global default program\n",
    "    feed_copy = feed_dict.copy()\n",
    "    feed_copy[graph.column_hidden_vectors] = np.zeros(\n",
    "        graph.column_hidden_vectors.get_shape().as_list())\n",
    "    feed_copy[graph.word_column_hidden_vectors] = np.zeros(\n",
    "        graph.word_column_hidden_vectors.get_shape().as_list())\n",
    "    default_operation_softmax, default_column_softmax = sess.run([graph.final_operation_softmax, graph.final_column_softmax], feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset + batch_id]\n",
    "\n",
    "        # get op indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # generate scaled feed\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size: # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0 / num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        \n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "        numeric_column_name_jump = [None] * graph.num_cols\n",
    "        word_column_name_jump = [None] * graph.num_word_cols\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            scaled_numeric_column_names = []\n",
    "            scaled_word_column_names = []\n",
    "\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy column\n",
    "                    scaled_numeric_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = scale * \\\n",
    "                        np.array(column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_numeric_column_names.append(np.expand_dims([np.array(\n",
    "                        column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_word_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = scale * \\\n",
    "                        np.array(word_column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_word_column_names.append(np.expand_dims([np.array(\n",
    "                        word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = 0\n",
    "\n",
    "            scaled_feed[graph.column_hidden_vectors] = np.concatenate(\n",
    "                scaled_numeric_column_names, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.word_column_hidden_vectors] = np.concatenate(\n",
    "                scaled_word_column_names, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax,\n",
    "                       graph.operator_gradients_default_program, graph.column_gradients_default_program]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(operator_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(column_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [actual_num_numeric_cols + actual_num_word_cols, 2 * graph.max_passes])\n",
    "        \n",
    "        row_labels = []  # column headers\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for i in range(actual_num_numeric_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "            \n",
    "        for i in range(actual_num_word_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.word_column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "\n",
    "        non_dummy_indices = np.arange(actual_num_numeric_cols).tolist() + (np.arange(actual_num_word_cols) + graph.num_cols).tolist()\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, non_dummy_indices]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, non_dummy_indices]\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_attrs.txt'), attributions_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on perturbed tables\n",
    "- Perturbed data is arranged such that unperturbed questions appear before perturbed questions. This results in words being added in the same order to the vocab (to effect in same word IDs) as in the unperturbed case.\n",
    "- Since the vocabulary has more words in the perturbed case (due to some words exceeding the min cutoff), special words such as `unk_token` are assigned different IDs. We revert this by swapping word IDs appropriately. The goal being that the word IDs of words in perturbed case should be the same as word IDs of words in the unperturbed case. This is done in `notebook_utils.init_data()` using the argument `preserve_vocab` (default value is `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Annotated examples loaded ', 58141)\n",
      "('Annotated examples loaded ', 4344)\n",
      "('entry match token: ', 9169, 9169)\n",
      "('entry match token: ', 9170, 9170)\n",
      "hardcoded ids for special words\n",
      "9133 9134 9135 9136\n",
      "('# train examples ', 43835)\n",
      "('# dev examples ', 10994)\n",
      "('# test examples ', 3913)\n"
     ]
    }
   ],
   "source": [
    "perturbed_train_data, perturbed_dev_data, perturbed_test_data, perturbed_utility = notebook_utils.init_data(PERTURBED_DATA_DIR, preserve_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.2556773688332028)\n",
      "(2554, 10994)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "perturbed_correct, perturbed_num_examples, perturbed_correct_dict = evaluate(sess, perturbed_dev_data, perturbed_utility.FLAGS.batch_size, graph, 92500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23066054397739313"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_correct/2831"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question concatenation attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_PHRASES = [\n",
    "    'in not a lot of words',\n",
    "    'in this chart',\n",
    "    'among these rows listed',\n",
    "    'if its all the same',\n",
    "    'above all',\n",
    "    'at the moment'\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_correct_list = []\n",
    "for phrase in ATTACK_PHRASES:\n",
    "    prefix_correct, _ = notebook_utils.evaluate_concatenation_attack(sess, dev_data, graph.batch_size, graph, 92500, utility, phrase, suffix=False)\n",
    "    suffix_correct, _ = notebook_utils.evaluate_concatenation_attack(sess, dev_data, graph.batch_size, graph, 92500, utility, phrase, suffix=True)\n",
    "    num_correct_list.append([prefix_correct, suffix_correct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word deletion attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = [\n",
    "    'how', 'tell', 'did', 'me', 'my', 'our', 'are', 'is', 'were', 'this', 'on',\n",
    "    'would', 'and', 'for', 'should', 'be', 'do', 'i', 'have', 'had', 'the',\n",
    "    'there', 'look', 'give', 'has', 'was', 'we', 'get', 'does', 'a', 'an', 's',\n",
    "    'that', 'by', 'based', 'in', 'of', 'bring', 'with', 'to', 'from',\n",
    "    'whole', 'being', 'been', 'want', 'wanted', 'as', 'can', 'see',\n",
    "    'doing', 'got', 'sorted', 'draw', 'listed', 'chart', 'only'\n",
    "]\n",
    "\n",
    "stop_word_ids = set([utility.word_ids[w] for w in STOP_WORDS if w in utility.word_ids])\n",
    "question_attention_mask_value = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_data = copy.deepcopy(dev_data)\n",
    "for i, wiki_example in enumerate(stop_word_data):\n",
    "    new_question = []\n",
    "    new_question_mask = []\n",
    "    for w in wiki_example.question:\n",
    "        if w not in stop_word_ids and w != utility.dummy_token_id:\n",
    "            new_question.append(w)\n",
    "            new_question_mask.append(0)\n",
    "                \n",
    "    stop_word_data[i].question = [utility.dummy_token_id] * (graph.question_length - len(new_question)) + new_question\n",
    "\n",
    "    stop_word_data[i].question_attention_mask = [question_attention_mask_value] * (graph.question_length - len(new_question_mask)) + new_question_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct, _ = evaluate(sess, stop_word_data, graph.batch_size, graph, 92500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
