{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Neural Programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "import autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, Image, clear_output, display\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sys.path.append('../neural_programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "import data_utils\n",
    "from neural_programmer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths, parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one GPU on the multi-GPU machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# WikiTableQuestions data\n",
    "DATA_DIR = '../wtq_data'\n",
    "PERTURBED_DATA_DIR = '../perturbed_wtq_data'\n",
    "\n",
    "# Pretrained model\n",
    "MODEL_FILE = os.path.join('..', 'pretrained_model', 'model_92500')\n",
    "\n",
    "# Output directory to write attributions\n",
    "OUT_DIR = '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, build graph and restore pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated examples loaded  14152\n",
      "Annotated examples loaded  4344\n",
      "entry match token:  9133 9133\n",
      "entry match token:  9134 9134\n",
      "# train examples  10178\n",
      "# dev examples  2546\n",
      "# test examples  3913\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, utility = notebook_utils.init_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget gate bias\n",
      "step:  0\n",
      "step:  1\n",
      "step:  2\n",
      "step:  3\n",
      "optimize params  ['unit', 'word', 'word_match_feature_column_name', 'controller', 'column_controller', 'column_controller_prev', 'controller_prev', 'question_lstm_ix', 'question_lstm_fx', 'question_lstm_cx', 'question_lstm_ox', 'question_lstm_im', 'question_lstm_fm', 'question_lstm_cm', 'question_lstm_om', 'question_lstm_i', 'question_lstm_f', 'question_lstm_c', 'question_lstm_o', 'history_recurrent', 'history_recurrent_bias', 'break_conditional']\n",
      "grads:  Tensor(\"gradients_24/L2Loss_grad/mul:0\", shape=(15, 256), dtype=float64) unit\n",
      "grads:  Tensor(\"gradients_24/L2Loss_1_grad/mul:0\", shape=(10800, 256), dtype=float64) word\n",
      "grads:  Tensor(\"gradients_24/L2Loss_2_grad/mul:0\", shape=(1,), dtype=float64) word_match_feature_column_name\n",
      "grads:  Tensor(\"gradients_24/L2Loss_3_grad/mul:0\", shape=(512, 256), dtype=float64) controller\n",
      "grads:  Tensor(\"gradients_24/L2Loss_4_grad/mul:0\", shape=(512, 256), dtype=float64) column_controller\n",
      "grads:  Tensor(\"gradients_24/L2Loss_5_grad/mul:0\", shape=(256, 256), dtype=float64) column_controller_prev\n",
      "grads:  Tensor(\"gradients_24/L2Loss_6_grad/mul:0\", shape=(256, 256), dtype=float64) controller_prev\n",
      "grads:  Tensor(\"gradients_24/L2Loss_7_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_ix\n",
      "grads:  Tensor(\"gradients_24/L2Loss_8_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_fx\n",
      "grads:  Tensor(\"gradients_24/L2Loss_9_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_cx\n",
      "grads:  Tensor(\"gradients_24/L2Loss_10_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_ox\n",
      "grads:  Tensor(\"gradients_24/L2Loss_11_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_im\n",
      "grads:  Tensor(\"gradients_24/L2Loss_12_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_fm\n",
      "grads:  Tensor(\"gradients_24/L2Loss_13_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_cm\n",
      "grads:  Tensor(\"gradients_24/L2Loss_14_grad/mul:0\", shape=(256, 256), dtype=float64) question_lstm_om\n",
      "grads:  Tensor(\"gradients_24/L2Loss_15_grad/mul:0\", shape=(256,), dtype=float64) question_lstm_i\n",
      "grads:  Tensor(\"gradients_24/L2Loss_16_grad/mul:0\", shape=(256,), dtype=float64) question_lstm_f\n",
      "grads:  Tensor(\"gradients_24/L2Loss_17_grad/mul:0\", shape=(256,), dtype=float64) question_lstm_c\n",
      "grads:  Tensor(\"gradients_24/L2Loss_18_grad/mul:0\", shape=(256,), dtype=float64) question_lstm_o\n",
      "grads:  Tensor(\"gradients_24/L2Loss_19_grad/mul:0\", shape=(768, 256), dtype=float64) history_recurrent\n",
      "grads:  Tensor(\"gradients_24/L2Loss_20_grad/mul:0\", shape=(1, 256), dtype=float64) history_recurrent_bias\n",
      "grads:  Tensor(\"gradients_24/L2Loss_21_grad/mul:0\", shape=(512, 256), dtype=float64) break_conditional\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess, graph, params = notebook_utils.build_graph(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../pretrained_model/model_92500\n"
     ]
    }
   ],
   "source": [
    "sess, graph = notebook_utils.restore_model(sess, graph, params, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev set accuracy   after  92500  :  0.3720472446811481\n",
      "2540.0 2546\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "evaluate(sess, dev_data, utility.FLAGS.batch_size, graph, 92500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients (IG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.mkdir(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 500\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 'prev', 'first', 'print']\n",
      "['team', 'team', 'wins', 'team']\n",
      "k: 0\n",
      "k: 20\n",
      "k: 40\n",
      "k: 60\n",
      "k: 80\n",
      "k: 100\n",
      "k: 120\n",
      "k: 140\n",
      "k: 160\n",
      "k: 180\n",
      "k: 200\n",
      "k: 220\n",
      "k: 240\n",
      "k: 260\n",
      "k: 280\n",
      "k: 300\n",
      "k: 320\n",
      "k: 340\n",
      "k: 360\n",
      "k: 380\n",
      "k: 400\n",
      "k: 420\n",
      "k: 440\n",
      "k: 460\n",
      "k: 480\n",
      "OP 0 : baseline= 0.18709323784186127 , input_fn= 0.9895706953516668 check:  0.8022113  -  0.8024774575098055  =  -0.00026617287235430886\n",
      "OP 1 : baseline= 0.000754526421322655 , input_fn= 0.9590392317258396 check:  0.9605387  -  0.9582847053045169  =  0.0022539800172909352\n",
      "OP 2 : baseline= 0.0032642627771376923 , input_fn= 0.9986749874390007 check:  0.9976185  -  0.9954107246618631  =  0.0022077717561361787\n",
      "OP 3 : baseline= 0.9975274390468174 , input_fn= 0.9999996349762448 check:  0.0024711965  -  0.0024721959294273788  =  -9.994642331534465e-07\n",
      "COL 0 : baseline= 0.0059443340639456456 , input_fn= 0.9998552390240457 check:  0.99448097  -  0.9939109049601  =  0.0005700625615674415\n",
      "COL 1 : baseline= 0.0045525129306662184 , input_fn= 0.5224792493280775 check:  0.5123967  -  0.5179267363974114  =  -0.005530043167736087\n",
      "COL 2 : baseline= 0.6793269732378839 , input_fn= 0.9981744154850433 check:  0.3188877  -  0.31884744224715933  =  4.0268324129733024e-05\n",
      "COL 3 : baseline= 0.24016438358473963 , input_fn= 0.9999524334960029 check:  0.7608238  -  0.7597880499112633  =  0.0010357363474342574\n"
     ]
    }
   ],
   "source": [
    "batch_size = graph.batch_size\n",
    "\n",
    "for offset in range(0, len(data) - graph.batch_size + 1, graph.batch_size):\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, graph.batch_size, graph)\n",
    "\n",
    "    # first run inference to get operator and column sequences, and embeddings of question words\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.question_words_embeddings]\n",
    "    correct_list, operation_softmax, column_softmax, question_words_embeddings = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute table-specific default programs for tables in this batch\n",
    "    feed_copy = feed_dict.copy()\n",
    "    for t in graph.question_words_embeddings:\n",
    "        feed_copy[t] = np.concatenate(\n",
    "            [np.expand_dims(dummy_embedding, 0)]*batch_size, 0)\n",
    "\n",
    "    # Ideally the following line should be uncommented, but for attributions,\n",
    "    # we choose to keep this variable fixed. Note that this induces some bias\n",
    "    # in the attributions as the baseline is no longer an \"empty\" question, but\n",
    "    # an empty question where the question length is implicitly encoded in this variable\n",
    "    # feed_copy[graph.batch_question_attention_mask].fill(question_attention_mask_value)\n",
    "\n",
    "    feed_copy[graph.batch_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_exact_match])\n",
    "    feed_copy[graph.batch_column_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_column_exact_match])\n",
    "\n",
    "    fetches = [graph.final_operation_softmax, graph.final_column_softmax]\n",
    "\n",
    "    default_operation_softmax, default_column_softmax = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset+batch_id]\n",
    "\n",
    "        # get operator indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        default_op_list = notebook_utils.softmax_to_names(\n",
    "            default_operation_softmax[0, :, :], utility.operations_set)\n",
    "        default_col_list = notebook_utils.softmax_to_names(\n",
    "            default_column_softmax[0, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # Sample points along the integral path and collect them as one batch\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:  # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0/num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "\n",
    "        exact_match = wiki_example.exact_match\n",
    "        exact_column_match = wiki_example.exact_column_match\n",
    "\n",
    "        batch_question_embeddings = np.array(question_words_embeddings)[\n",
    "            :, batch_id, :]  # shape: 62 x 256\n",
    "\n",
    "        # split up set of points into batch_size'd batches\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            # scale question words to points between dummy_embedding and actual embedding\n",
    "            qw_jump = [None]*graph.question_length\n",
    "            for i, t in enumerate(graph.question_words_embeddings):\n",
    "                qw_jump[i] = scale * \\\n",
    "                    (batch_question_embeddings[i] - dummy_embedding)\n",
    "                scaled_feed[t] = [dummy_embedding + j*qw_jump[i]\n",
    "                                  for j in range(k, k+batch_size)]\n",
    "\n",
    "            # scale batch_exact_match\n",
    "            scaled_exact_match = []\n",
    "            scaled_column_exact_match = []\n",
    "\n",
    "            exact_match_jump = [None]*(graph.num_cols + graph.num_word_cols)\n",
    "            exact_column_match_jump = [None] * \\\n",
    "                (graph.num_cols + graph.num_word_cols)\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy columns\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = scale*np.array(exact_match[i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = scale * \\\n",
    "                        np.array(exact_column_match[i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[graph.num_cols+i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_match[graph.num_cols+i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[graph.num_cols + i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_column_match[graph.num_cols + i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[graph.num_cols+i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[graph.num_cols + i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = 0\n",
    "\n",
    "            scaled_feed[graph.batch_exact_match] = np.concatenate(\n",
    "                scaled_exact_match, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.batch_column_exact_match] = np.concatenate(\n",
    "                scaled_column_exact_match, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax, graph.operator_gradients,\n",
    "                       graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([operator_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([operator_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([column_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([column_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check to make sure the integral summation adds up to function difference\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "        question_begin = np.nonzero(\n",
    "            wiki_example.question_attention_mask)[0].shape[0]\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [graph.question_length - question_begin + 2, 2 * graph.max_passes])\n",
    "        row_labels = []  # question words, tm, cm\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for ix in range(question_begin, graph.question_length):\n",
    "            word = utility.reverse_word_ids[wiki_example.question[ix]]\n",
    "            if word == utility.unk_token:\n",
    "                word = word + '-' + [str(w) for w in wiki_example.string_question if w !=\n",
    "                                     wiki_example.question_number and w != wiki_example.question_number_1][ix - question_begin]\n",
    "            word = notebook_utils.rename(word)\n",
    "            row_labels.append(word)\n",
    "        row_labels.extend(['tm', 'cm'])\n",
    "\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, question_begin:]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, question_begin:]\n",
    "\n",
    "        question_string = ' '.join([notebook_utils.rename(str(w))\n",
    "                                    for w in wiki_example.string_question])\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write(question_string)\n",
    "            outf.write('\\n')\n",
    "            outf.write(str(correct_list[batch_id] == 1.0))\n",
    "            outf.write('\\n')\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'), attributions_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HTML with visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients on table-specific default programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this file\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions_default_programs')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.mkdir(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 500\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all unique tables\n",
    "unique_tables = {}\n",
    "for wiki_example in data:\n",
    "    if not wiki_example.table_key in unique_tables:\n",
    "        wiki_example.exact_column_match = np.zeros_like(\n",
    "            wiki_example.exact_column_match).tolist()\n",
    "        wiki_example.exact_match = np.zeros_like(\n",
    "            wiki_example.exact_match).tolist()\n",
    "        wiki_example.question = [\n",
    "            utility.dummy_token_id] * graph.question_length\n",
    "        wiki_example.question_attention_mask = (question_attention_mask_value * \\\n",
    "            np.ones_like(wiki_example.question_attention_mask)).tolist()\n",
    "        unique_tables[wiki_example.table_key] = wiki_example\n",
    "data = list(unique_tables.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for offset in range(0, len(data) - graph.batch_size + 1, batch_size):\n",
    "\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, batch_size, graph)\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.column_hidden_vectors, graph.word_column_hidden_vectors]\n",
    "    correct_list, operation_softmax, column_softmax, column_hidden_vectors, word_column_hidden_vectors = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute global default program\n",
    "    feed_copy = feed_dict.copy()\n",
    "    feed_copy[graph.column_hidden_vectors] = np.zeros(\n",
    "        graph.column_hidden_vectors.get_shape().as_list())\n",
    "    feed_copy[graph.word_column_hidden_vectors] = np.zeros(\n",
    "        graph.word_column_hidden_vectors.get_shape().as_list())\n",
    "    _, default_operation_softmax, default_column_softmax, _, _ = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset + batch_id]\n",
    "\n",
    "        # get op indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], get_column_names(wiki_example))\n",
    "\n",
    "        print op_list\n",
    "        print col_list\n",
    "\n",
    "        # generate scaled feed\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in scaled_feed.keys():\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = 25 * batch_size\n",
    "        scale = 1.0 / num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "        numeric_column_name_jump = [None] * graph.num_cols\n",
    "        word_column_name_jump = [None] * graph.num_word_cols\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print 'k:', k\n",
    "            scaled_numeric_column_names = []\n",
    "            scaled_word_column_names = []\n",
    "\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy column\n",
    "                    scaled_numeric_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = scale * \\\n",
    "                        np.array(column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_numeric_column_names.append(np.expand_dims([np.array(\n",
    "                        column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_word_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = scale * \\\n",
    "                        np.array(word_column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_word_column_names.append(np.expand_dims([np.array(\n",
    "                        word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = 0\n",
    "\n",
    "            scaled_feed[graph.column_hidden_vectors] = np.concatenate(\n",
    "                scaled_numeric_column_names, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.word_column_hidden_vectors] = np.concatenate(\n",
    "                scaled_word_column_names, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax,\n",
    "                       graph.operator_gradients, graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = len(operator_gradients) / graph.max_passes\n",
    "                temp = [np.sum(operator_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(operator_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = len(column_gradients) / graph.max_passes\n",
    "                temp = [np.sum(column_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(column_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print 'OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print 'COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs\n",
    "\n",
    "        op_attributions = [None] * graph.max_passes\n",
    "        for stage in range(graph.max_passes):\n",
    "            cumsum_attr = np.cumsum(\n",
    "                np.sort(np.abs(batch_op_attribution[stage]))[::-1])\n",
    "            total_attr = cumsum_attr[-1]\n",
    "            cumsum_attr = (1.0 * cumsum_attr) / \\\n",
    "                total_attr if total_attr > 0.0 else 0.0 * cumsum_attr\n",
    "            if total_attr > 0.0:\n",
    "                take_attr = min(num_attributions, np.nonzero(cumsum_attr > attribution_coverage)[\n",
    "                                0][0] + 1, len(batch_op_attribution[0]))\n",
    "#            take_attr = min(num_attributions, len(batch_op_attribution[0]))\n",
    "            else:\n",
    "                take_attr = 0\n",
    "            attr_indices = np.argsort(np.abs(batch_op_attribution[stage]))[\n",
    "                ::-1][:take_attr]\n",
    "            attrs = [\n",
    "                'baseline (' + str(round(attr_op_softmax[0, stage, op_indices[stage]], 6)) + ')<br>']\n",
    "            for ix in attr_indices:\n",
    "                attr_value = batch_op_attribution[stage][ix]\n",
    "                if ix < graph.num_cols:\n",
    "                    word = utility.reverse_word_ids[wiki_example.column_ids[ix][0]]\n",
    "                else:\n",
    "                    word = utility.reverse_word_ids[wiki_example.word_column_ids[ix-graph.num_cols][0]]\n",
    "                attrs.append('<b>' + word + '</b>')\n",
    "                attrs[-1] += '(' + str(round(attr_value, 3)) + ')'\n",
    "            op_attributions[stage] = ', '.join(attrs)\n",
    "\n",
    "        col_attributions = [None] * graph.max_passes\n",
    "        for stage in range(graph.max_passes):\n",
    "            cumsum_attr = np.cumsum(\n",
    "                np.sort(np.abs(batch_col_attribution[stage]))[::-1])\n",
    "            total_attr = cumsum_attr[-1]\n",
    "            cumsum_attr = (1.0 * cumsum_attr) / \\\n",
    "                total_attr if total_attr > 0.0 else 0.0 * cumsum_attr\n",
    "            if total_attr > 0.0:\n",
    "                take_attr = min(num_attributions, np.nonzero(cumsum_attr > attribution_coverage)[\n",
    "                                0][0] + 1, len(batch_col_attribution[0]))\n",
    "#            take_attr = min(num_attributions, len(batch_col_attribution[0]))\n",
    "            else:\n",
    "                take_attr = 0\n",
    "            attr_indices = np.argsort(np.abs(batch_col_attribution[stage]))[\n",
    "                ::-1][:take_attr]\n",
    "            attrs = [\n",
    "                'baseline (' + str(round(attr_col_softmax[0, stage, col_indices[stage]], 6)) + ')<br>']\n",
    "            for ix in attr_indices:\n",
    "                attr_value = batch_col_attribution[stage][ix]\n",
    "                if ix < graph.num_cols:\n",
    "                    word = utility.reverse_word_ids[wiki_example.column_ids[ix][0]]\n",
    "                else:\n",
    "                    word = utility.reverse_word_ids[wiki_example.word_column_ids[ix-graph.num_cols][0]]\n",
    "                attrs.append('<b>' + word + '</b>')\n",
    "                attrs[-1] += '(' + str(round(attr_value, 3)) + ')'\n",
    "            col_attributions[stage] = ', '.join(attrs)\n",
    "\n",
    "        question_string = ' '.join([str(w)\n",
    "                                    for w in wiki_example.string_question])\n",
    "        if correct_list[batch_id] == 1.0:\n",
    "            question_string = '<font color=\"green\">' + question_string + '</font>'\n",
    "        else:\n",
    "            question_string = '<font color=\"red\">' + question_string + '</font>'\n",
    "\n",
    "        question_id = '<a href=https://wikitables.googleplex.com/table?table_name=' + \\\n",
    "            wiki_example.table_key[4:] + '&version=np_eval_on_unshuffled_data_of_original_model_trained_on_unshuffled_data>' + \\\n",
    "            wiki_example.question_id + '</a>'\n",
    "        default_op_list = softmax_to_names(\n",
    "            default_operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        default_col_list = softmax_to_names(\n",
    "            default_column_softmax[batch_id, :, :], get_column_names(wiki_example))\n",
    "        print default_op_list\n",
    "        print default_col_list\n",
    "\n",
    "        combined_op_list = []\n",
    "        combined_col_list = []\n",
    "        stage = 0\n",
    "        for op, dop in zip(op_list, default_op_list):\n",
    "            dop_softmax_final = default_operation_softmax[batch_id,\n",
    "                                                          stage, op_indices[stage]]\n",
    "            op_line = op\n",
    "            op_line += '<br><font color=\"green\">' + dop + '</font>' + \\\n",
    "                '(' + str(round(dop_softmax_final, 3)) + ')'\n",
    "            combined_op_list.append(op_line)\n",
    "            stage += 1\n",
    "\n",
    "        stage = 0\n",
    "        for col, dcol in zip(col_list, default_col_list):\n",
    "            dcol_softmax_final = default_column_softmax[batch_id,\n",
    "                                                        stage, col_indices[stage]]\n",
    "            col_line = col\n",
    "            col_line += '<br><font color=\"green\">' + dcol + '</font>' + \\\n",
    "                '(' + str(round(dcol_softmax_final, 3)) + ')'\n",
    "            combined_col_list.append(col_line)\n",
    "            stage += 1\n",
    "\n",
    "        line = '\\t'.join([question_id, question_string] + op_attributions +\n",
    "                         col_attributions + combined_op_list + combined_col_list)\n",
    "        print line\n",
    "#        break\n",
    "#    break\n",
    "        outf.write(line + '\\n')\n",
    "        outf.flush()\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
