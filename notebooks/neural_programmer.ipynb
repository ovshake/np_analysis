{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Neural Programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "import autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, Image, clear_output, display\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sys.path.append('../neural_programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "import data_utils\n",
    "from neural_programmer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths, parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one GPU on the multi-GPU machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0-3\"\n",
    "\n",
    "# WikiTableQuestions data\n",
    "DATA_DIR = '../wtq_data'\n",
    "PERTURBED_DATA_DIR = '../perturbed_wtq_data'\n",
    "\n",
    "# Pretrained model\n",
    "MODEL_FILE = os.path.join('..', 'pretrained_model', 'model_92500')\n",
    "model_step = int(MODEL_FILE.split('_')[-1])\n",
    "\n",
    "# Output directory to write attributions\n",
    "OUT_DIR = '../results'\n",
    "\n",
    "# Overstability curve file\n",
    "OVERSTABILITY_CURVE_FILE = os.path.join(OUT_DIR, 'overstability.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators whose results builds upon the result of previously applied operators\n",
    "acts_on_prev_result = {\n",
    "    'count': True,\n",
    "    'prev': True,\n",
    "    'next': True,\n",
    "    'first': True,\n",
    "    'last': True,\n",
    "    'mfe': False,\n",
    "    'greater': False,\n",
    "    'lesser': False,\n",
    "    'geq': False,\n",
    "    'leq': False,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': False,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "}\n",
    "\n",
    "# Operators whose result depends on the column it is acting on\n",
    "relies_on_col = {\n",
    "    'count': False,\n",
    "    'prev': False,\n",
    "    'next': False,\n",
    "    'first': False,\n",
    "    'last': False,\n",
    "    'mfe': True,\n",
    "    'greater': True,\n",
    "    'lesser': True,\n",
    "    'geq': True,\n",
    "    'leq': True,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': True,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "    \n",
    "}\n",
    "\n",
    "def get_program_mask(program):\n",
    "    \"\"\" \n",
    "    Returns a mask indicating attributions to which ops/cols are considered significant.\n",
    "    The conditions for are:\n",
    "    1) affect answer computation,\n",
    "    2) are not the same as their table-specific default counterparts\n",
    "    \n",
    "    program = [op (default_op), col (default_col)] * 4 \n",
    "    \"\"\"\n",
    "    mask = [False] * (2 * 4)\n",
    "    for i in range(3, -1, -1):\n",
    "        op, default_op = program[2*i].split('(')\n",
    "        op = op.strip()\n",
    "        default_op = default_op.strip().strip(')')\n",
    "        mask[2*i] = (op != default_op)\n",
    "        col, default_col = program[2*i + 1].split('(')\n",
    "        col = col.strip()\n",
    "        default_col = default_col.strip().strip(')')\n",
    "        if relies_on_col[op]:\n",
    "            mask[2*i+1] = (col != default_col)\n",
    "        if not acts_on_prev_result[op]:\n",
    "            break\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, build graph and restore pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Annotated examples loaded ', 14152)\n",
      "('Annotated examples loaded ', 4344)\n",
      "('entry match token: ', 9133, 9133)\n",
      "('entry match token: ', 9134, 9134)\n",
      "('# train examples ', 10178)\n",
      "('# dev examples ', 2546)\n",
      "('# test examples ', 3913)\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, utility, unprocessed_dev_data = notebook_utils.init_data(DATA_DIR)\n",
    "num_dev_examples = 2831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget gate bias\n",
      "('step: ', 0)\n",
      "('step: ', 1)\n",
      "('step: ', 2)\n",
      "('step: ', 3)\n",
      "('optimize params ', ['unit', 'word', 'word_match_feature_column_name', 'controller', 'column_controller', 'column_controller_prev', 'controller_prev', 'question_lstm_ix', 'question_lstm_fx', 'question_lstm_cx', 'question_lstm_ox', 'question_lstm_im', 'question_lstm_fm', 'question_lstm_cm', 'question_lstm_om', 'question_lstm_i', 'question_lstm_f', 'question_lstm_c', 'question_lstm_o', 'history_recurrent', 'history_recurrent_bias', 'break_conditional'])\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_grad/mul:0' shape=(15, 256) dtype=float64>, 'unit')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_1_grad/mul:0' shape=(10800, 256) dtype=float64>, 'word')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_2_grad/mul:0' shape=(1,) dtype=float64>, 'word_match_feature_column_name')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_3_grad/mul:0' shape=(512, 256) dtype=float64>, 'controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_4_grad/mul:0' shape=(512, 256) dtype=float64>, 'column_controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_5_grad/mul:0' shape=(256, 256) dtype=float64>, 'column_controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_6_grad/mul:0' shape=(256, 256) dtype=float64>, 'controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_7_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ix')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_8_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_9_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_10_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ox')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_11_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_im')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_12_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_13_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_14_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_om')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_15_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_i')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_16_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_f')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_17_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_c')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_18_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_o')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_19_grad/mul:0' shape=(768, 256) dtype=float64>, 'history_recurrent')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_20_grad/mul:0' shape=(1, 256) dtype=float64>, 'history_recurrent_bias')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_21_grad/mul:0' shape=(512, 256) dtype=float64>, 'break_conditional')\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess, graph, params = notebook_utils.build_graph(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../pretrained_model/model_92500\n"
     ]
    }
   ],
   "source": [
    "sess, graph = notebook_utils.restore_model(sess, graph, params, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.37195600942655144)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "num_correct, num_examples, correct_dict = evaluate(sess, dev_data, utility.FLAGS.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3345107735782409"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct/float(num_dev_examples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients (IG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = graph.batch_size\n",
    "\n",
    "for offset in range(0, len(data) - graph.batch_size + 1, graph.batch_size):\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, graph.batch_size, graph)\n",
    "\n",
    "    # first run inference to get operator and column sequences, and embeddings of question words\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.question_words_embeddings]\n",
    "    correct_list, operation_softmax, column_softmax, question_words_embeddings = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute table-specific default programs for tables in this batch\n",
    "    feed_copy = feed_dict.copy()\n",
    "    for t in graph.question_words_embeddings:\n",
    "        feed_copy[t] = np.concatenate(\n",
    "            [np.expand_dims(dummy_embedding, 0)]*batch_size, 0)\n",
    "\n",
    "    # Ideally the following line should be uncommented, but for attributions,\n",
    "    # we choose to keep this variable fixed. Note that this induces some bias\n",
    "    # in the attributions as the baseline is no longer an \"empty\" question, but\n",
    "    # an empty question where the question length is implicitly encoded in this variable\n",
    "    # feed_copy[graph.batch_question_attention_mask].fill(question_attention_mask_value)\n",
    "\n",
    "    feed_copy[graph.batch_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_exact_match])\n",
    "    feed_copy[graph.batch_column_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_column_exact_match])\n",
    "\n",
    "    fetches = [graph.final_operation_softmax, graph.final_column_softmax]\n",
    "\n",
    "    default_operation_softmax, default_column_softmax = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset+batch_id]\n",
    "\n",
    "        # get operator indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        default_op_list = notebook_utils.softmax_to_names(\n",
    "            default_operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        default_col_list = notebook_utils.softmax_to_names(\n",
    "            default_column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # Sample points along the integral path and collect them as one batch\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:  # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0/num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "\n",
    "        exact_match = wiki_example.exact_match\n",
    "        exact_column_match = wiki_example.exact_column_match\n",
    "\n",
    "        batch_question_embeddings = np.array(question_words_embeddings)[\n",
    "            :, batch_id, :]  # shape: 62 x 256\n",
    "\n",
    "        # split up set of points into batch_size'd batches\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            # scale question words to points between dummy_embedding and actual embedding\n",
    "            qw_jump = [None]*graph.question_length\n",
    "            for i, t in enumerate(graph.question_words_embeddings):\n",
    "                qw_jump[i] = scale * \\\n",
    "                    (batch_question_embeddings[i] - dummy_embedding)\n",
    "                scaled_feed[t] = [dummy_embedding + j*qw_jump[i]\n",
    "                                  for j in range(k, k+batch_size)]\n",
    "\n",
    "            # scale batch_exact_match\n",
    "            scaled_exact_match = []\n",
    "            scaled_column_exact_match = []\n",
    "\n",
    "            exact_match_jump = [None]*(graph.num_cols + graph.num_word_cols)\n",
    "            exact_column_match_jump = [None] * \\\n",
    "                (graph.num_cols + graph.num_word_cols)\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy columns\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = scale*np.array(exact_match[i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = scale * \\\n",
    "                        np.array(exact_column_match[i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[graph.num_cols+i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_match[graph.num_cols+i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[graph.num_cols + i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_column_match[graph.num_cols + i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[graph.num_cols+i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[graph.num_cols + i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = 0\n",
    "\n",
    "            scaled_feed[graph.batch_exact_match] = np.concatenate(\n",
    "                scaled_exact_match, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.batch_column_exact_match] = np.concatenate(\n",
    "                scaled_column_exact_match, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax, graph.operator_gradients,\n",
    "                       graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([operator_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([operator_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([column_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([column_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check to make sure the integral summation adds up to function difference\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "        question_begin = np.nonzero(\n",
    "            wiki_example.question_attention_mask)[0].shape[0]\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [graph.question_length - question_begin + 2, 2 * graph.max_passes])\n",
    "        row_labels = []  # question words, tm, cm\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for ix in range(question_begin, graph.question_length):\n",
    "            word = utility.reverse_word_ids[wiki_example.question[ix]]\n",
    "            if word == utility.unk_token:\n",
    "                word = word + '-' + [str(w) for w in wiki_example.string_question if w !=\n",
    "                                     wiki_example.question_number and w != wiki_example.question_number_1][ix - question_begin]\n",
    "            word = notebook_utils.rename(word)\n",
    "            row_labels.append(word)\n",
    "        row_labels.extend(['tm', 'cm'])\n",
    "\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, question_begin:]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, question_begin:]\n",
    "\n",
    "        question_string = ' '.join([notebook_utils.rename(str(w))\n",
    "                                    for w in wiki_example.string_question])\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write(question_string)\n",
    "            outf.write('\\n')\n",
    "            outf.write(str(correct_list[batch_id] == 1.0))\n",
    "            outf.write('\\n')\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'), attributions_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HTML with visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "figs_outdir = os.path.join(OUT_DIR, \"heatmaps\")\n",
    "if not os.path.isdir(figs_outdir):\n",
    "    os.makedirs(figs_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)\n",
    "rc={'axes.labelsize': 11, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    attributions = np.loadtxt(os.path.join(attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        lines = f.readlines()\n",
    "        xlabels = ['\\n'.join(w.split()) for w in lines[3].strip().split('\\t')]\n",
    "        ylabels = lines[2].strip().split('\\t')\n",
    "    mask = get_program_mask(lines[3].strip().split('\\t'))\n",
    "    mask = np.expand_dims(mask, 0)\n",
    "    plt.figure(figsize=(len(xlabels),len(ylabels)/2))\n",
    "    plot_data = attributions/attributions.sum(axis=0)*mask\n",
    "    with sns.axes_style('white'):\n",
    "        sns.heatmap(plot_data, cbar=False, xticklabels=xlabels, yticklabels=ylabels, annot=True, fmt='.2f', robust=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figs_outdir, wiki_example.question_id + '.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations written to ../results/visualizations.html\n"
     ]
    }
   ],
   "source": [
    "with tf.gfile.GFile(os.path.join(OUT_DIR, 'visualizations.html'), 'w') as htmlf:\n",
    "    html_str = '<head><link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\"></head>'\n",
    "    html_str += '<body> <div class=\"container\"> <h3> Visualizations of the attributions for the Neural Programmer network <br> <small> Lighter colors indicate high values <br> Green and red questions indicate whether the network got the answer right (or wrong)</small></h3></div><br>'\n",
    "    html_str += '<div class=\"container\">'\n",
    "    for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "                lines = f.readlines()\n",
    "                html_str += wiki_example.question_id + ' <div class=' + ('\"text-success\"' if lines[1].strip() == 'True' else '\"text-danger\"') + '>' + lines[0] + '</div><br>'\n",
    "                html_str += '<img src=\"heatmaps/' + wiki_example.question_id + '.png\"></img><br><hr><br>'\n",
    "    \n",
    "    html_str += '</div></body></html>'\n",
    "    htmlf.write(html_str)\n",
    "    print(\"Visualizations written to\",os.path.join(OUT_DIR, 'visualizations.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "operator_triggers = defaultdict(lambda: [])\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    mask = get_program_mask(col_labels)\n",
    "    for i in range(4):\n",
    "        if not mask[2*i]:\n",
    "            continue\n",
    "        syn = [row_labels[j] for j in np.argpartition(attrs[:, 2*i], -K)[-K:]]\n",
    "        syn = [utility.unk_token if s.startswith(utility.unk_token) else s for s in syn]\n",
    "        operator_triggers[col_labels[2*i].split('(')[0].strip()] += syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operator</th>\n",
       "      <th>Triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>select</td>\n",
       "      <td>[tm_token, many, how, number, or, total, after, before, only]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prev</td>\n",
       "      <td>[before, many, than, previous, above, how, at, most]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>[tm_token, first, before, after, who, previous, or, peak]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reset</td>\n",
       "      <td>[many, total, how, number, last, least, the, first, of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>count</td>\n",
       "      <td>[many, how, number, total, of, difference, between, long, times]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>next</td>\n",
       "      <td>[after, not, many, next, same, tm_token, how, below]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last</td>\n",
       "      <td>[last, or, after, tm_token, next, the, chart, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mfe</td>\n",
       "      <td>[most, cm_token, same, as, many, tm_token, not, players, how]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>min</td>\n",
       "      <td>[least, the, not, same, did, amount, smallest, as]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>[most, largest, tallest, taller, building, highest, has, had]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>geq</td>\n",
       "      <td>[at, more, least, had, over, number, than, many, have, above]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>print</td>\n",
       "      <td>[tm_token, cm_token, or, most, release, before, players]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Operator                                                          Triggers\n",
       "0    select     [tm_token, many, how, number, or, total, after, before, only]\n",
       "1      prev              [before, many, than, previous, above, how, at, most]\n",
       "2     first         [tm_token, first, before, after, who, previous, or, peak]\n",
       "3     reset           [many, total, how, number, last, least, the, first, of]\n",
       "4     count  [many, how, number, total, of, difference, between, long, times]\n",
       "5      next              [after, not, many, next, same, tm_token, how, below]\n",
       "6      last                [last, or, after, tm_token, next, the, chart, not]\n",
       "7       mfe     [most, cm_token, same, as, many, tm_token, not, players, how]\n",
       "8       min                [least, the, not, same, did, amount, smallest, as]\n",
       "9       max     [most, largest, tallest, taller, building, highest, has, had]\n",
       "10      geq     [at, more, least, had, over, number, than, many, have, above]\n",
       "11    print          [tm_token, cm_token, or, most, release, before, players]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "df_data = [[k, [w for w, _ in Counter(v).most_common(K) if w not in ['tm','cm']]] for k, v in operator_triggers.items()]\n",
    "df = pd.DataFrame(df_data, columns=['Operator', 'Triggers'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "  Operator &                                                          Triggers \\\\\n",
      "\\midrule\n",
      " \\opselect &     [tm_token, many, how, number, or, total, after, before, only] \\\\\n",
      "   \\opprev &              [before, many, than, previous, above, how, at, most] \\\\\n",
      "  \\opfirst &         [tm_token, first, before, after, who, previous, or, peak] \\\\\n",
      "  \\opreset &           [many, total, how, number, last, least, the, first, of] \\\\\n",
      "  \\opcount &  [many, how, number, total, of, difference, between, long, times] \\\\\n",
      "   \\opnext &              [after, not, many, next, same, tm_token, how, below] \\\\\n",
      "   \\oplast &                [last, or, after, tm_token, next, the, chart, not] \\\\\n",
      "    \\opmfe &     [most, cm_token, same, as, many, tm_token, not, players, how] \\\\\n",
      "    \\opmin &                [least, the, not, same, did, amount, smallest, as] \\\\\n",
      "    \\opmax &     [most, largest, tallest, taller, building, highest, has, had] \\\\\n",
      "    \\opgeq &     [at, more, least, had, over, number, than, many, have, above] \\\\\n",
      "  \\opprint &          [tm_token, cm_token, or, most, release, before, players] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['Operator'] = '\\op' + df['Operator']\n",
    "print(df.to_latex(escape=False, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients on table-specific default programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this file\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions_default_programs')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = copy.deepcopy(dev_data)\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all unique tables\n",
    "unique_tables = {}\n",
    "for wiki_example in data:\n",
    "    if not wiki_example.table_key in unique_tables:\n",
    "        wiki_example.exact_column_match = np.zeros_like(\n",
    "            wiki_example.exact_column_match).tolist()\n",
    "        wiki_example.exact_match = np.zeros_like(\n",
    "            wiki_example.exact_match).tolist()\n",
    "        wiki_example.question = [\n",
    "            utility.dummy_token_id] * graph.question_length\n",
    "        wiki_example.question_attention_mask = (question_attention_mask_value * \\\n",
    "            np.ones_like(wiki_example.question_attention_mask)).tolist()\n",
    "        unique_tables[wiki_example.table_key] = wiki_example\n",
    "data = list(unique_tables.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(0, len(data) - graph.batch_size + 1, batch_size):\n",
    "\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, batch_size, graph)\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.column_hidden_vectors, graph.word_column_hidden_vectors]\n",
    "    correct_list, operation_softmax, column_softmax, column_hidden_vectors, word_column_hidden_vectors = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute global default program\n",
    "    feed_copy = feed_dict.copy()\n",
    "    feed_copy[graph.column_hidden_vectors] = np.zeros(\n",
    "        graph.column_hidden_vectors.get_shape().as_list())\n",
    "    feed_copy[graph.word_column_hidden_vectors] = np.zeros(\n",
    "        graph.word_column_hidden_vectors.get_shape().as_list())\n",
    "    default_operation_softmax, default_column_softmax = sess.run([graph.final_operation_softmax, graph.final_column_softmax], feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset + batch_id]\n",
    "\n",
    "        # get op indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # generate scaled feed\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size: # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0 / num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        \n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "        numeric_column_name_jump = [None] * graph.num_cols\n",
    "        word_column_name_jump = [None] * graph.num_word_cols\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            scaled_numeric_column_names = []\n",
    "            scaled_word_column_names = []\n",
    "\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy column\n",
    "                    scaled_numeric_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = scale * \\\n",
    "                        np.array(column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_numeric_column_names.append(np.expand_dims([np.array(\n",
    "                        column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_word_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = scale * \\\n",
    "                        np.array(word_column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_word_column_names.append(np.expand_dims([np.array(\n",
    "                        word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = 0\n",
    "\n",
    "            scaled_feed[graph.column_hidden_vectors] = np.concatenate(\n",
    "                scaled_numeric_column_names, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.word_column_hidden_vectors] = np.concatenate(\n",
    "                scaled_word_column_names, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax,\n",
    "                       graph.operator_gradients_default_program, graph.column_gradients_default_program]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(operator_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(column_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [actual_num_numeric_cols + actual_num_word_cols, 2 * graph.max_passes])\n",
    "        \n",
    "        row_labels = []  # column headers\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for i in range(actual_num_numeric_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "            \n",
    "        for i in range(actual_num_word_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.word_column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "\n",
    "        non_dummy_indices = np.arange(actual_num_numeric_cols).tolist() + (np.arange(actual_num_word_cols) + graph.num_cols).tolist()\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, non_dummy_indices]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, non_dummy_indices]\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_attrs.txt'), attributions_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common triggers for table-specific default program operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(unique_tables.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "../results/attributions_default_programs/201_28_labels.tsv; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-373-d4dfc01728ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwiki_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs_outdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_table_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_labels.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrow_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcol_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line from the file. Leaves the '\\n' at the end.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadLineAsString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 80\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ../results/attributions_default_programs/201_28_labels.tsv; No such file or directory"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "program_triggers = defaultdict(lambda: [])\n",
    "program_counts = defaultdict(int)\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_labels.tsv')) as f:\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_attrs.txt'))\n",
    "    \n",
    "    synonyms = []\n",
    "    for i, stage in enumerate(col_labels):\n",
    "        synonyms.extend([row_labels[j] for j in np.argpartition(attrs[:, i], -K)[-K:]])\n",
    "    \n",
    "    program = ', '.join([c.split()[0] for i, c in enumerate(col_labels) if i%2 == 0])\n",
    "    program_triggers[program] = program_triggers[program] + synonyms       \n",
    "    program_counts[program] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=10000\n",
    "K = 5\n",
    "df_data = []\n",
    "for program, triggers in program_triggers.items():\n",
    "    topk = Counter(triggers).most_common(K)\n",
    "    df_data.append([program, program_counts[program], [w for w, _ in topk]])\n",
    "df_data = sorted(df_data, key=operator.itemgetter(1), reverse=True)\n",
    "df = pd.DataFrame(df_data, columns=['Operator sequence', '#tables', 'Triggers'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrl}\n",
      "\\toprule\n",
      "            Operator sequence &  #tables &                                                                              Triggers \\\\\n",
      "\\midrule\n",
      "     reset, reset, max, print &      109 &          [UNK, points, position, name, year, venue, competition, date, notes, player] \\\\\n",
      "      reset, prev, max, print &       68 &                      [UNK, total, rank, silver, nation, bronze, year, name, no, gold] \\\\\n",
      "   reset, reset, first, print &       29 &      [UNK, name, rank, notes, year, location, nationality, comments, previous, party] \\\\\n",
      "     reset, mfe, first, print &       25 &                    [notes, title, date, UNK, role, year, genre, label, score, format] \\\\\n",
      "     reset, reset, min, print &       17 &             [year, height, name, UNK, position, floors, jan, single, location, chart] \\\\\n",
      "       reset, mfe, max, print &       14 &              [opponent, result, rank, date, location, notes, name, peak, city, score] \\\\\n",
      "    reset, next, first, print &       10 &              [UNK, name, time, edition, year, birth, definition, death, manner, type] \\\\\n",
      "    reset, reset, last, print &        9 &  [UNK, distance, location, name, years, year, peak, time, certifications, university] \\\\\n",
      "    reset, prev, first, print &        7 &  [name, UNK, time, athlete, rank, location, intersecting, kilometers, notes, service] \\\\\n",
      "      reset, next, max, print &        7 &                                                                      [UNK, ethnicity] \\\\\n",
      "      reset, mfe, last, print &        6 &                 [UNK, place, season, tier, date, division, country, time, note, rank] \\\\\n",
      "     reset, prev, last, print &        5 &                    [report, date, east, votes, season, UNK, race, notes, works, name] \\\\\n",
      "      reset, prev, min, print &        4 &     [level, movements, season, joined, division, notes, gauge, position, works, type] \\\\\n",
      "   reset, select, last, print &        3 &           [rank, finish, start, location, coordinates, year, laps, UNK, car, carries] \\\\\n",
      "   reset, reset, first, count &        2 &                                                    [programming, UNK, network, owner] \\\\\n",
      "   reset, reset, reset, print &        1 &                               [candidates, result, first, party, district, incumbent] \\\\\n",
      " reset, select, select, print &        1 &                                         [name, lifetime, notes, nationality, notable] \\\\\n",
      "    reset, reset, last, count &        1 &                                                  [japanese, type, year, english, UNK] \\\\\n",
      "     reset, next, last, print &        1 &                                                                        [UNK, comment] \\\\\n",
      "    reset, mfe, select, print &        1 &                                             [producer, title, performer, length, UNK] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False,escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on perturbed tables\n",
    "- Perturbed data is arranged such that unperturbed questions appear before perturbed questions. This results in words being added in the same order to the vocab (to effect in same word IDs) as in the unperturbed case.\n",
    "- Since the vocabulary has more words in the perturbed case (due to some words exceeding the min cutoff), special words such as `unk_token` are assigned different IDs. We revert this by swapping word IDs appropriately. The goal being that the word IDs of words in perturbed case should be the same as word IDs of words in the unperturbed case. This is done in `notebook_utils.init_data()` using the argument `preserve_vocab` (default value is `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_train_data, perturbed_dev_data, perturbed_test_data, perturbed_utility, _ = notebook_utils.init_data(PERTURBED_DATA_DIR, preserve_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_correct, perturbed_num_examples, perturbed_correct_dict = evaluate(sess, perturbed_dev_data, perturbed_utility.FLAGS.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_correct/num_dev_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question concatenation attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_PHRASES = [\n",
    "    'in not a lot of words',\n",
    "    'in this chart',\n",
    "    'among these rows listed',\n",
    "    'if its all the same',\n",
    "    'above all',\n",
    "    'at the moment',\n",
    "    'in not many words',\n",
    "    'before we proceed',\n",
    "    'after all',\n",
    "    'last question'\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.2293794186959937)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.11115475255302436)\n",
      "(2546, 2546)\n",
      "--------\n",
      "584 283\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3448546739984289)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.33032207384131973)\n",
      "(2546, 2546)\n",
      "--------\n",
      "878 841\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3479968578161822)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3271798900235664)\n",
      "(2546, 2546)\n",
      "--------\n",
      "886 833\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.24234092694422624)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.20777690494893952)\n",
      "(2546, 2546)\n",
      "--------\n",
      "617 529\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.28868813825608797)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.22191673212882954)\n",
      "(2546, 2546)\n",
      "--------\n",
      "735 565\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3527101335428123)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.30597014925373134)\n",
      "(2546, 2546)\n",
      "--------\n",
      "898 779\n"
     ]
    }
   ],
   "source": [
    "num_correct_list = []\n",
    "concatenation_correct_dicts = []\n",
    "for phrase in ATTACK_PHRASES:\n",
    "    # prefix\n",
    "    qc_attack_data = copy.deepcopy(unprocessed_dev_data)\n",
    "    for wiki_example in qc_attack_data:\n",
    "        if not correct_dict[wiki_example.question_id]:\n",
    "            continue\n",
    "        wiki_example.question = phrase.strip().split() + wiki_example.question\n",
    "    qc_attack_data = data_utils.complete_wiki_processing(qc_attack_data, utility, train=False)\n",
    "    \n",
    "    prefix_correct, _, prefix_correct_dict = evaluate(sess, qc_attack_data, graph.batch_size, graph, model_step)\n",
    "\n",
    "    \n",
    "    # suffix\n",
    "    qc_attack_data = copy.deepcopy(unprocessed_dev_data)\n",
    "    for wiki_example in qc_attack_data:\n",
    "        if not correct_dict[wiki_example.question_id]:\n",
    "            continue\n",
    "        wiki_example.question = wiki_example.question + phrase.strip().split()\n",
    "    qc_attack_data = data_utils.complete_wiki_processing(qc_attack_data, utility, train=False)\n",
    "\n",
    "    suffix_correct, _, suffix_correct_dict = evaluate(sess, qc_attack_data, graph.batch_size, graph, model_step)\n",
    "\n",
    "    \n",
    "    print(prefix_correct, suffix_correct)\n",
    "    num_correct_list.append([prefix_correct, suffix_correct])\n",
    "    concatenation_correct_dicts.append([prefix_correct_dict, suffix_correct_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate([np.expand_dims(ATTACK_PHRASES,1), np.array(num_correct_list)/num_dev_examples], axis=1), columns=['Attack phrase', 'Prefix','Suffix'])\n",
    "df.Prefix = df.Prefix.apply(lambda x: str(round(100*float(x), 1)) + '%')\n",
    "df['Suffix'] = df['Suffix'].apply(lambda x: str(round(100*float(x), 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "           Attack phrase & Prefix & Suffix \\\\\n",
      "\\midrule\n",
      "   in not a lot of words &  20.6\\% &  10.0\\% \\\\\n",
      "           in this chart &  31.0\\% &  29.7\\% \\\\\n",
      " among these rows listed &  31.3\\% &  29.4\\% \\\\\n",
      "     if its all the same &  21.8\\% &  18.7\\% \\\\\n",
      "               above all &  26.0\\% &  20.0\\% \\\\\n",
      "           at the moment &  31.7\\% &  27.5\\% \\\\\n",
      "       in not many words &  15.6\\% &  11.2\\% \\\\\n",
      "       before we proceed &  25.5\\% &  17.8\\% \\\\\n",
      "               after all &  23.3\\% &  14.6\\% \\\\\n",
      "           last question &  27.2\\% &  24.5\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False, escape=True, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of all attacks: 0.032497350759448956\n"
     ]
    }
   ],
   "source": [
    "combined_dicts = correct_dict.copy()\n",
    "\n",
    "for [prefix_results, suffix_results] in concatenation_correct_dicts:\n",
    "    for k, v in combined_dicts.items():\n",
    "        combined_dicts[k] = v and prefix_results[k] and suffix_results[k]\n",
    "\n",
    "print(\"Union of all attacks:\", sum(combined_dicts.values())/num_dev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word deletion attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = \"\"\"\n",
    "                show, tell, did,  me, you, your, my, \n",
    "                our, are,  is, at, were, this, on, would, \n",
    "                and,  for, should,  be, do, I, have, had, \n",
    "                the, there, has,  was, we, get, does, a,  \n",
    "                an,  s,  that,  by,  based, in,  of, bring,\n",
    "                with, to, from, whole, been,  want, wanted,\n",
    "                as, than, got, sorted, draw, listed, chart, \n",
    "                only\n",
    "            \"\"\"\n",
    "STOP_WORDS = STOP_WORDS.strip().split(', ')\n",
    "STOP_WORDS = [w.strip() for w in STOP_WORDS]\n",
    "STOP_WORDS = set(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_data = copy.deepcopy(unprocessed_dev_data)\n",
    "for i, wiki_example in enumerate(stop_word_data):\n",
    "    if not correct_dict[wiki_example.question_id]:\n",
    "        continue\n",
    "    stop_word_data[i].question = [w for w in wiki_example.question if w not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_data = data_utils.complete_wiki_processing(stop_word_data, utility, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.3173605655930872)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "num_correct, _, stop_word_correct_dict = evaluate(sess, stop_word_data, graph.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28541151536559517"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct/num_dev_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overstability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_attributions = []\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    \n",
    "    synonyms = []\n",
    "    program_mask = get_program_mask(col_labels)\n",
    "    for i, stage in enumerate(col_labels):\n",
    "        if not program_mask[i]:\n",
    "            continue\n",
    "        syn = row_labels[np.argmax(attrs[:, i])]\n",
    "        if syn.startswith(utility.unk_token):\n",
    "            syn = utility.unk_token\n",
    "        synonyms.append(syn)\n",
    "    \n",
    "    frequent_attributions.extend(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tm', 1391),\n",
       " ('many', 1037),\n",
       " ('cm', 827),\n",
       " ('number', 446),\n",
       " ('tm_token', 281),\n",
       " ('after', 253),\n",
       " ('total', 191),\n",
       " ('before', 175),\n",
       " ('or', 140),\n",
       " ('last', 132)]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(frequent_attributions).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.12293794186959937)\n",
      "(2546, 2546)\n",
      "--------\n",
      "1 313\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.13275726630007856)\n",
      "(2546, 2546)\n",
      "--------\n",
      "2 338\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.18460329929300864)\n",
      "(2546, 2546)\n",
      "--------\n",
      "3 470\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.20267085624509035)\n",
      "(2546, 2546)\n",
      "--------\n",
      "4 516\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.22309505106048705)\n",
      "(2546, 2546)\n",
      "--------\n",
      "7 568\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.23409269442262373)\n",
      "(2546, 2546)\n",
      "--------\n",
      "9 596\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.24548311076197957)\n",
      "(2546, 2546)\n",
      "--------\n",
      "12 625\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2498036135113904)\n",
      "(2546, 2546)\n",
      "--------\n",
      "15 636\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.25530243519245877)\n",
      "(2546, 2546)\n",
      "--------\n",
      "19 650\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2737627651217596)\n",
      "(2546, 2546)\n",
      "--------\n",
      "24 697\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2827965435978005)\n",
      "(2546, 2546)\n",
      "--------\n",
      "30 720\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.29025923016496463)\n",
      "(2546, 2546)\n",
      "--------\n",
      "37 739\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.30479183032207385)\n",
      "(2546, 2546)\n",
      "--------\n",
      "56 776\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3079340141398272)\n",
      "(2546, 2546)\n",
      "--------\n",
      "68 784\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3150039277297722)\n",
      "(2546, 2546)\n",
      "--------\n",
      "84 802\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3267871170463472)\n",
      "(2546, 2546)\n",
      "--------\n",
      "102 832\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.32953652788688137)\n",
      "(2546, 2546)\n",
      "--------\n",
      "125 839\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3330714846818539)\n",
      "(2546, 2546)\n",
      "--------\n",
      "153 848\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.33385703063629224)\n",
      "(2546, 2546)\n",
      "--------\n",
      "186 850\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3362136684996072)\n",
      "(2546, 2546)\n",
      "--------\n",
      "228 856\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3582089552238806)\n",
      "(2546, 2546)\n",
      "--------\n",
      "278 912\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3601728201099764)\n",
      "(2546, 2546)\n",
      "--------\n",
      "339 917\n"
     ]
    }
   ],
   "source": [
    "utility.word_ids['tm_token'] = utility.entry_match_token_id\n",
    "utility.word_ids['cm_token'] = utility.column_match_token_id\n",
    "curve_data = {}\n",
    "for K in np.unique(np.floor(np.geomspace(3, len(Counter(frequent_attributions)), 25))):\n",
    "    whitelist = set([w for w, _ in Counter(frequent_attributions).most_common(int(K))])\n",
    "    \n",
    "    # 'tm', and 'cm' are non-words\n",
    "    whitelist.remove('tm')\n",
    "    whitelist.remove('cm')\n",
    "    \n",
    "    whitelist = set([utility.word_ids[w] for w in whitelist if not w.startswith(utility.unk_token)])\n",
    "    \n",
    "    if len(whitelist) in curve_data:\n",
    "        continue\n",
    "\n",
    "    reduced_vocab_data = copy.deepcopy(dev_data)\n",
    "    for i, wiki_example in enumerate(reduced_vocab_data):\n",
    "        new_question = []\n",
    "        for w in wiki_example.question:\n",
    "            if w in whitelist:\n",
    "                new_question.append(w)\n",
    "        reduced_vocab_data[i].question = [utility.dummy_token_id] * (graph.question_length - len(new_question)) + new_question\n",
    "        reduced_vocab_data[i].question_attention_mask = [question_attention_mask_value] * (graph.question_length - len(new_question)) + [0] * len(new_question)\n",
    "    \n",
    "    num_correct, _, _ = evaluate(sess, reduced_vocab_data, graph.batch_size, graph, model_step)\n",
    "    curve_data[len(whitelist)] = num_correct\n",
    "    print(len(whitelist), num_correct)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(curve_data.keys(), np.divide(list(curve_data.values()), num_dev_examples))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('num. words in vocab')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
