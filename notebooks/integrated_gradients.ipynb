{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Neural Programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import spacy\n",
    "import autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, Image, clear_output, display\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sys.path.append('../neural_programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "import data_utils\n",
    "from neural_programmer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths, parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one GPU on the multi-GPU machine\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "\n",
    "# WikiTableQuestions data\n",
    "DATA_DIR = '../wtq_data'\n",
    "PERTURBED_DATA_DIR = '../perturbed_wtq_data'\n",
    "\n",
    "# Pretrained model\n",
    "MODEL_FILE = os.path.join('..', 'pretrained_model', 'model_92500')\n",
    "model_step = int(MODEL_FILE.split('_')[-1])\n",
    "\n",
    "# Output directory to write attributions\n",
    "OUT_DIR = '../results'\n",
    "\n",
    "# Overstability curve file\n",
    "OVERSTABILITY_CURVE_FILE = os.path.join(OUT_DIR, 'overstability.eps')\n",
    "\n",
    "pd.options.display.max_colwidth=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators whose results builds upon the result of previously applied operators\n",
    "acts_on_prev_result = {\n",
    "    'count': True,\n",
    "    'prev': True,\n",
    "    'next': True,\n",
    "    'first': True,\n",
    "    'last': True,\n",
    "    'mfe': False,\n",
    "    'greater': False,\n",
    "    'lesser': False,\n",
    "    'geq': False,\n",
    "    'leq': False,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': False,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "}\n",
    "\n",
    "# Operators whose result depends on the column it is acting on\n",
    "relies_on_col = {\n",
    "    'count': False,\n",
    "    'prev': False,\n",
    "    'next': False,\n",
    "    'first': False,\n",
    "    'last': False,\n",
    "    'mfe': True,\n",
    "    'greater': True,\n",
    "    'lesser': True,\n",
    "    'geq': True,\n",
    "    'leq': True,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': True,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "    \n",
    "}\n",
    "\n",
    "def get_program_mask(program, ignore_answer_cond=False):\n",
    "    \"\"\" \n",
    "    Returns a mask indicating attributions to which ops/cols are considered significant.\n",
    "    The conditions for are:\n",
    "    1) affect answer computation, (toggled by \"ignore_answer_cond\")\n",
    "    2) are not the same as their table-specific default counterparts\n",
    "    \n",
    "    program = [op (default_op), col (default_col)] * 4 \n",
    "    \"\"\"\n",
    "    mask = [False] * (2 * 4)\n",
    "    for i in range(3, -1, -1):\n",
    "        op, default_op = program[2*i].split('(')\n",
    "        op = op.strip()\n",
    "        default_op = default_op.strip().strip(')')\n",
    "        mask[2*i] = (op != default_op)\n",
    "        col, default_col = program[2*i + 1].split('(')\n",
    "        col = col.strip()\n",
    "        default_col = default_col.strip().strip(')')\n",
    "        if ignore_answer_cond:\n",
    "            continue\n",
    "        if relies_on_col[op]:\n",
    "            mask[2*i+1] = (col != default_col)\n",
    "        if not acts_on_prev_result[op]:\n",
    "            break\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, build graph and restore pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Annotated examples loaded ', 14152)\n",
      "('Annotated examples loaded ', 4344)\n",
      "('entry match token: ', 9133, 9133)\n",
      "('entry match token: ', 9134, 9134)\n",
      "('# train examples ', 10178)\n",
      "('# dev examples ', 2546)\n",
      "('# test examples ', 3913)\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, utility, unprocessed_dev_data = notebook_utils.init_data(DATA_DIR)\n",
    "num_dev_examples = 2831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget gate bias\n",
      "('step: ', 0)\n",
      "('step: ', 1)\n",
      "('step: ', 2)\n",
      "('step: ', 3)\n",
      "('optimize params ', ['unit', 'word', 'word_match_feature_column_name', 'controller', 'column_controller', 'column_controller_prev', 'controller_prev', 'question_lstm_ix', 'question_lstm_fx', 'question_lstm_cx', 'question_lstm_ox', 'question_lstm_im', 'question_lstm_fm', 'question_lstm_cm', 'question_lstm_om', 'question_lstm_i', 'question_lstm_f', 'question_lstm_c', 'question_lstm_o', 'history_recurrent', 'history_recurrent_bias', 'break_conditional'])\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_grad/mul:0' shape=(15, 256) dtype=float64>, 'unit')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_1_grad/mul:0' shape=(10800, 256) dtype=float64>, 'word')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_2_grad/mul:0' shape=(1,) dtype=float64>, 'word_match_feature_column_name')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_3_grad/mul:0' shape=(512, 256) dtype=float64>, 'controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_4_grad/mul:0' shape=(512, 256) dtype=float64>, 'column_controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_5_grad/mul:0' shape=(256, 256) dtype=float64>, 'column_controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_6_grad/mul:0' shape=(256, 256) dtype=float64>, 'controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_7_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ix')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_8_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_9_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_10_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ox')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_11_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_im')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_12_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_13_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_14_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_om')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_15_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_i')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_16_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_f')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_17_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_c')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_18_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_o')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_19_grad/mul:0' shape=(768, 256) dtype=float64>, 'history_recurrent')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_20_grad/mul:0' shape=(1, 256) dtype=float64>, 'history_recurrent_bias')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_21_grad/mul:0' shape=(512, 256) dtype=float64>, 'break_conditional')\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess, graph, params = notebook_utils.build_graph(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../pretrained_model/model_92500\n"
     ]
    }
   ],
   "source": [
    "sess, graph = notebook_utils.restore_model(sess, graph, params, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.37195600942655144)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "num_correct, num_examples, correct_dict = evaluate(sess, dev_data, utility.FLAGS.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3345107735782409\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation accuracy:\", num_correct/float(num_dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients (IG) \n",
    "Note: attributions are available already in \"results\". Hence, to reproduce the results of the ACL paper, one can skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = graph.batch_size\n",
    "\n",
    "for offset in range(0, len(data) - graph.batch_size + 1, graph.batch_size):\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, graph.batch_size, graph)\n",
    "\n",
    "    # first run inference to get operator and column sequences, and embeddings of question words\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.question_words_embeddings]\n",
    "    correct_list, operation_softmax, column_softmax, question_words_embeddings = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute table-specific default programs for tables in this batch\n",
    "    feed_copy = feed_dict.copy()\n",
    "    for t in graph.question_words_embeddings:\n",
    "        feed_copy[t] = np.concatenate(\n",
    "            [np.expand_dims(dummy_embedding, 0)]*batch_size, 0)\n",
    "\n",
    "    # Ideally the following line should be uncommented, but for attributions,\n",
    "    # we choose to keep this variable fixed. Note that this induces some bias\n",
    "    # in the attributions as the baseline is no longer an \"empty\" question, but\n",
    "    # an empty question where the question length is implicitly encoded in this variable\n",
    "    # feed_copy[graph.batch_question_attention_mask].fill(question_attention_mask_value)\n",
    "\n",
    "    feed_copy[graph.batch_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_exact_match])\n",
    "    feed_copy[graph.batch_column_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_column_exact_match])\n",
    "\n",
    "    fetches = [graph.final_operation_softmax, graph.final_column_softmax]\n",
    "\n",
    "    default_operation_softmax, default_column_softmax = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset+batch_id]\n",
    "\n",
    "        # get operator indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        default_op_list = notebook_utils.softmax_to_names(\n",
    "            default_operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        default_col_list = notebook_utils.softmax_to_names(\n",
    "            default_column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # Sample points along the integral path and collect them as one batch\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:  # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0/num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "\n",
    "        exact_match = wiki_example.exact_match\n",
    "        exact_column_match = wiki_example.exact_column_match\n",
    "\n",
    "        batch_question_embeddings = np.array(question_words_embeddings)[\n",
    "            :, batch_id, :]  # shape: 62 x 256\n",
    "\n",
    "        # split up set of points into batch_size'd batches\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            # scale question words to points between dummy_embedding and actual embedding\n",
    "            qw_jump = [None]*graph.question_length\n",
    "            for i, t in enumerate(graph.question_words_embeddings):\n",
    "                qw_jump[i] = scale * \\\n",
    "                    (batch_question_embeddings[i] - dummy_embedding)\n",
    "                scaled_feed[t] = [dummy_embedding + j*qw_jump[i]\n",
    "                                  for j in range(k, k+batch_size)]\n",
    "\n",
    "            # scale batch_exact_match\n",
    "            scaled_exact_match = []\n",
    "            scaled_column_exact_match = []\n",
    "\n",
    "            exact_match_jump = [None]*(graph.num_cols + graph.num_word_cols)\n",
    "            exact_column_match_jump = [None] * \\\n",
    "                (graph.num_cols + graph.num_word_cols)\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy columns\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = scale*np.array(exact_match[i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = scale * \\\n",
    "                        np.array(exact_column_match[i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[graph.num_cols+i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_match[graph.num_cols+i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[graph.num_cols + i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_column_match[graph.num_cols + i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[graph.num_cols+i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[graph.num_cols + i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = 0\n",
    "\n",
    "            scaled_feed[graph.batch_exact_match] = np.concatenate(\n",
    "                scaled_exact_match, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.batch_column_exact_match] = np.concatenate(\n",
    "                scaled_column_exact_match, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax, graph.operator_gradients,\n",
    "                       graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([operator_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([operator_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([column_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([column_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check to make sure the integral summation adds up to function difference\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "        question_begin = np.nonzero(\n",
    "            wiki_example.question_attention_mask)[0].shape[0]\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [graph.question_length - question_begin + 2, 2 * graph.max_passes])\n",
    "        row_labels = []  # question words, tm, cm\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for ix in range(question_begin, graph.question_length):\n",
    "            word = utility.reverse_word_ids[wiki_example.question[ix]]\n",
    "            if word == utility.unk_token:\n",
    "                word = word + '-' + [str(w) for w in wiki_example.string_question if w !=\n",
    "                                     wiki_example.question_number and w != wiki_example.question_number_1][ix - question_begin]\n",
    "            word = notebook_utils.rename(word)\n",
    "            row_labels.append(word)\n",
    "        row_labels.extend(['tm', 'cm'])\n",
    "\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, question_begin:]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, question_begin:]\n",
    "\n",
    "        question_string = ' '.join([notebook_utils.rename(str(w))\n",
    "                                    for w in wiki_example.string_question])\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write(question_string)\n",
    "            outf.write('\\n')\n",
    "            outf.write(str(correct_list[batch_id] == 1.0))\n",
    "            outf.write('\\n')\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'), attributions_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HTML with visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "figs_outdir = os.path.join(OUT_DIR, \"heatmaps\")\n",
    "if not os.path.isdir(figs_outdir):\n",
    "    os.makedirs(figs_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)\n",
    "rc={'axes.labelsize': 11, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    attributions = np.loadtxt(os.path.join(attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        lines = f.readlines()\n",
    "        xlabels = ['\\n'.join(w.split()) for w in lines[3].strip().split('\\t')]\n",
    "        ylabels = lines[2].strip().split('\\t')\n",
    "    mask = get_program_mask(lines[3].strip().split('\\t'))\n",
    "    mask = np.expand_dims(mask, 0)\n",
    "    plt.figure(figsize=(len(xlabels),len(ylabels)/2))\n",
    "    plot_data = attributions/attributions.sum(axis=0)*mask\n",
    "    with sns.axes_style('white'):\n",
    "        sns.heatmap(plot_data, cbar=False, xticklabels=xlabels, yticklabels=ylabels, annot=True, fmt='.2f', robust=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figs_outdir, wiki_example.question_id + '.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations written to ../results/visualizations.html\n"
     ]
    }
   ],
   "source": [
    "with tf.gfile.GFile(os.path.join(OUT_DIR, 'visualizations.html'), 'w') as htmlf:\n",
    "    html_str = '<head><link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\"></head>'\n",
    "    html_str += '<body> <div class=\"container\"> <h3> Visualizations of the attributions for the Neural Programmer network <br> <small> Lighter colors indicate high values <br> Green and red questions indicate whether the network got the answer right (or wrong)</small></h3></div><br>'\n",
    "    html_str += '<div class=\"container\">'\n",
    "    for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "                lines = f.readlines()\n",
    "                html_str += wiki_example.question_id + ' <div class=' + ('\"text-success\"' if lines[1].strip() == 'True' else '\"text-danger\"') + '>' + lines[0] + '</div><br>'\n",
    "                html_str += '<img src=\"heatmaps/' + wiki_example.question_id + '.png\"></img><br><hr><br>'\n",
    "    \n",
    "    html_str += '</div></body></html>'\n",
    "    htmlf.write(html_str)\n",
    "    print(\"Visualizations written to\",os.path.join(OUT_DIR, 'visualizations.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "data = dev_data\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "operator_triggers = defaultdict(lambda: [])\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    mask = get_program_mask(col_labels)\n",
    "    for i in range(4):\n",
    "        if not mask[2*i]:\n",
    "            continue\n",
    "        syn = [row_labels[j] for j in np.argpartition(attrs[:, 2*i], -K)[-K:]]\n",
    "        syn = [utility.unk_token if s.startswith(utility.unk_token) else s for s in syn]\n",
    "        operator_triggers[col_labels[2*i].split('(')[0].strip()] += syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operator</th>\n",
       "      <th>Triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>select</td>\n",
       "      <td>[tm_token, many, how, number, or, total, after, before, only]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prev</td>\n",
       "      <td>[before, many, than, previous, above, how, at, most]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>[tm_token, first, before, after, who, previous, or, peak]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reset</td>\n",
       "      <td>[many, total, how, number, last, least, the, first, of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>count</td>\n",
       "      <td>[many, how, number, total, of, difference, between, long, times]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>next</td>\n",
       "      <td>[after, not, many, next, same, tm_token, how, below]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last</td>\n",
       "      <td>[last, or, after, tm_token, next, the, chart, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mfe</td>\n",
       "      <td>[most, cm_token, same]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>min</td>\n",
       "      <td>[least, the, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>[most, largest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>geq</td>\n",
       "      <td>[at, more, least, had, over, number, than, many]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>print</td>\n",
       "      <td>[tm_token]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Operator                                                          Triggers\n",
       "0    select     [tm_token, many, how, number, or, total, after, before, only]\n",
       "1      prev              [before, many, than, previous, above, how, at, most]\n",
       "2     first         [tm_token, first, before, after, who, previous, or, peak]\n",
       "3     reset           [many, total, how, number, last, least, the, first, of]\n",
       "4     count  [many, how, number, total, of, difference, between, long, times]\n",
       "5      next              [after, not, many, next, same, tm_token, how, below]\n",
       "6      last                [last, or, after, tm_token, next, the, chart, not]\n",
       "7       mfe                                            [most, cm_token, same]\n",
       "8       min                                                 [least, the, not]\n",
       "9       max                                                   [most, largest]\n",
       "10      geq                  [at, more, least, had, over, number, than, many]\n",
       "11    print                                                        [tm_token]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "L = 5\n",
    "df_data = [[k, [w for w, l in Counter(v).most_common(K) if w not in ['tm','cm'] and l > L]] for k, v in operator_triggers.items()]\n",
    "df = pd.DataFrame(df_data, columns=['Operator', 'Triggers'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "  Operator &                                                          Triggers \\\\\n",
      "\\midrule\n",
      " \\opselect &     [tm_token, many, how, number, or, total, after, before, only] \\\\\n",
      "   \\opprev &              [before, many, than, previous, above, how, at, most] \\\\\n",
      "  \\opfirst &         [tm_token, first, before, after, who, previous, or, peak] \\\\\n",
      "  \\opreset &           [many, total, how, number, last, least, the, first, of] \\\\\n",
      "  \\opcount &  [many, how, number, total, of, difference, between, long, times] \\\\\n",
      "   \\opnext &              [after, not, many, next, same, tm_token, how, below] \\\\\n",
      "   \\oplast &                [last, or, after, tm_token, next, the, chart, not] \\\\\n",
      "    \\opmfe &                                            [most, cm_token, same] \\\\\n",
      "    \\opmin &                                                 [least, the, not] \\\\\n",
      "    \\opmax &                                                   [most, largest] \\\\\n",
      "    \\opgeq &                  [at, more, least, had, over, number, than, many] \\\\\n",
      "  \\opprint &                                                        [tm_token] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['Operator'] = '\\op' + df['Operator']\n",
    "print(df.to_latex(escape=False, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate how frequently the default operator is the selected operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "num_matches = [0] * 4\n",
    "count = 0\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "for wiki_example in data[:utility.FLAGS.batch_size*int(len(data)/utility.FLAGS.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        f.readline()\n",
    "        is_correct = f.readline().strip()\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    program = [[w.strip(')') for w in rl.split(' (')] for i, rl in enumerate(col_labels) if i % 2 == 0]\n",
    "    num_matches  = np.add(num_matches, [w == dw for w, dw in program])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of times selected operators match original operators: 0.36929133858267715\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of times selected operators match original operators:\", np.mean(np.divide(num_matches,count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Integrated Gradients on table-specific default programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write attributions to this file\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions_default_programs')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = copy.deepcopy(dev_data)\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all unique tables\n",
    "unique_tables = {}\n",
    "for wiki_example in data:\n",
    "    if not wiki_example.table_key in unique_tables:\n",
    "        wiki_example.exact_column_match = np.zeros_like(\n",
    "            wiki_example.exact_column_match).tolist()\n",
    "        wiki_example.exact_match = np.zeros_like(\n",
    "            wiki_example.exact_match).tolist()\n",
    "        wiki_example.question = [\n",
    "            utility.dummy_token_id] * graph.question_length\n",
    "        wiki_example.question_attention_mask = (question_attention_mask_value * \\\n",
    "            np.ones_like(wiki_example.question_attention_mask)).tolist()\n",
    "        unique_tables[wiki_example.table_key] = wiki_example\n",
    "data = list(unique_tables.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(0, len(data) - graph.batch_size + 1, batch_size):\n",
    "\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, batch_size, graph)\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.column_hidden_vectors, graph.word_column_hidden_vectors]\n",
    "    correct_list, operation_softmax, column_softmax, column_hidden_vectors, word_column_hidden_vectors = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute global default program\n",
    "    feed_copy = feed_dict.copy()\n",
    "    feed_copy[graph.column_hidden_vectors] = np.zeros(\n",
    "        graph.column_hidden_vectors.get_shape().as_list())\n",
    "    feed_copy[graph.word_column_hidden_vectors] = np.zeros(\n",
    "        graph.word_column_hidden_vectors.get_shape().as_list())\n",
    "    default_operation_softmax, default_column_softmax = sess.run([graph.final_operation_softmax, graph.final_column_softmax], feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset + batch_id]\n",
    "\n",
    "        # get op indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # generate scaled feed\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size: # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0 / num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.num_cols + graph.num_word_cols], dtype=np.float32)\n",
    "        \n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "        numeric_column_name_jump = [None] * graph.num_cols\n",
    "        word_column_name_jump = [None] * graph.num_word_cols\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            scaled_numeric_column_names = []\n",
    "            scaled_word_column_names = []\n",
    "\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy column\n",
    "                    scaled_numeric_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = scale * \\\n",
    "                        np.array(column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_numeric_column_names.append(np.expand_dims([np.array(\n",
    "                        column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    numeric_column_name_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_word_column_names.append(np.expand_dims(\n",
    "                        [j * scale * np.array(word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = scale * \\\n",
    "                        np.array(word_column_hidden_vectors[batch_id, i, :])\n",
    "                else:\n",
    "                    scaled_word_column_names.append(np.expand_dims([np.array(\n",
    "                        word_column_hidden_vectors[batch_id, i, :]) for j in range(k, k + batch_size)], 1))\n",
    "                    word_column_name_jump[i] = 0\n",
    "\n",
    "            scaled_feed[graph.column_hidden_vectors] = np.concatenate(\n",
    "                scaled_numeric_column_names, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.word_column_hidden_vectors] = np.concatenate(\n",
    "                scaled_word_column_names, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax,\n",
    "                       graph.operator_gradients_default_program, graph.column_gradients_default_program]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(operator_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients) / graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n * stage][0][:, i, :] *\n",
    "                               numeric_column_name_jump[i]) for i in range(graph.num_cols)]\n",
    "                temp += [np.sum(column_gradients[n * stage + 1][0][:, i, :] *\n",
    "                                word_column_name_jump[i]) for i in range(graph.num_word_cols)]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=', input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs - rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [actual_num_numeric_cols + actual_num_word_cols, 2 * graph.max_passes])\n",
    "        \n",
    "        row_labels = []  # column headers\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for i in range(actual_num_numeric_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "            \n",
    "        for i in range(actual_num_word_cols):\n",
    "            word = utility.reverse_word_ids[wiki_example.word_column_ids[i][0]]\n",
    "            row_labels.append(word)\n",
    "\n",
    "        non_dummy_indices = np.arange(actual_num_numeric_cols).tolist() + (np.arange(actual_num_word_cols) + graph.num_cols).tolist()\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, non_dummy_indices]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, non_dummy_indices]\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_attrs.txt'), attributions_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common triggers for table-specific default program operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(unique_tables.values())\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions_default_programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "program_triggers = defaultdict(lambda: [])\n",
    "program_counts = defaultdict(int)\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_labels.tsv')) as f:\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "        \n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, notebook_utils.process_table_key(wiki_example.table_key) + '_attrs.txt'))\n",
    "    \n",
    "    synonyms = []\n",
    "    for i, stage in enumerate(col_labels):\n",
    "        if i % 2 != 0:\n",
    "            continue\n",
    "        synonyms.extend([row_labels[j] for j in np.argpartition(attrs[:, i], -K)[-K:]])\n",
    "    \n",
    "    program = ', '.join(['\\op' + c.split()[0] for i, c in enumerate(col_labels) if i%2 == 0])\n",
    "    program_triggers[program] = program_triggers[program] + np.unique(synonyms).tolist()\n",
    "    program_counts[program] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operator sequence</th>\n",
       "      <th>#tables</th>\n",
       "      <th>Triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\opreset, \\opreset, \\opmax, \\opprint</td>\n",
       "      <td>109</td>\n",
       "      <td>[UNK, date, position, points, name, competition, notes, no, year, venue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\opreset, \\opprev, \\opmax, \\opprint</td>\n",
       "      <td>68</td>\n",
       "      <td>[UNK, rank, total, bronze, gold, silver, nation, name, date, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\opreset, \\opreset, \\opfirst, \\opprint</td>\n",
       "      <td>29</td>\n",
       "      <td>[name, UNK, notes, year, nationality, rank, location, date, comments, hometown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\opreset, \\opmfe, \\opfirst, \\opprint</td>\n",
       "      <td>25</td>\n",
       "      <td>[notes, date, title, UNK, role, genre, year, score, opponent, event]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\opreset, \\opreset, \\opmin, \\opprint</td>\n",
       "      <td>17</td>\n",
       "      <td>[year, height, UNK, name, position, floors, notes, jan, jun, may]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\opreset, \\opmfe, \\opmax, \\opprint</td>\n",
       "      <td>14</td>\n",
       "      <td>[opponent, date, result, location, rank, site, attendance, notes, city, listing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\opreset, \\opnext, \\opfirst, \\opprint</td>\n",
       "      <td>10</td>\n",
       "      <td>[UNK, name, year, edition, birth, death, men, time, women, type]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\opreset, \\opreset, \\oplast, \\opprint</td>\n",
       "      <td>9</td>\n",
       "      <td>[date, UNK, distance, location, name, year, winner, japanese, duration, member]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\opreset, \\opprev, \\opfirst, \\opprint</td>\n",
       "      <td>7</td>\n",
       "      <td>[name, notes, intersecting, kilometers, location, athlete, nationality, rank, time, design]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\opreset, \\opnext, \\opmax, \\opprint</td>\n",
       "      <td>7</td>\n",
       "      <td>[UNK, ethnicity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\opreset, \\opmfe, \\oplast, \\opprint</td>\n",
       "      <td>6</td>\n",
       "      <td>[place, season, UNK, date, division, tier, builder, cylinders, notes, withdrawn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\opreset, \\opprev, \\oplast, \\opprint</td>\n",
       "      <td>5</td>\n",
       "      <td>[report, date, average, chassis, driver, race, builder, name, notes, works]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\opreset, \\opprev, \\opmin, \\opprint</td>\n",
       "      <td>4</td>\n",
       "      <td>[division, level, movements, position, season, current, gauge, notes, wheel, works]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\opreset, \\opselect, \\oplast, \\opprint</td>\n",
       "      <td>3</td>\n",
       "      <td>[car, finish, laps, led, rank, retired, start, year, UNK, carries]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\opreset, \\opreset, \\opfirst, \\opcount</td>\n",
       "      <td>2</td>\n",
       "      <td>[UNK, network, owner, programming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\opreset, \\opreset, \\opreset, \\opprint</td>\n",
       "      <td>1</td>\n",
       "      <td>[candidates, district, first, incumbent, party, result]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\opreset, \\opselect, \\opselect, \\opprint</td>\n",
       "      <td>1</td>\n",
       "      <td>[lifetime, name, nationality, notable, notes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\opreset, \\opreset, \\oplast, \\opcount</td>\n",
       "      <td>1</td>\n",
       "      <td>[UNK, english, japanese, type, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\opreset, \\opnext, \\oplast, \\opprint</td>\n",
       "      <td>1</td>\n",
       "      <td>[UNK, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\opreset, \\opmfe, \\opselect, \\opprint</td>\n",
       "      <td>1</td>\n",
       "      <td>[UNK, length, performer, producer, title]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Operator sequence  #tables  \\\n",
       "0       \\opreset, \\opreset, \\opmax, \\opprint      109   \n",
       "1        \\opreset, \\opprev, \\opmax, \\opprint       68   \n",
       "2     \\opreset, \\opreset, \\opfirst, \\opprint       29   \n",
       "3       \\opreset, \\opmfe, \\opfirst, \\opprint       25   \n",
       "4       \\opreset, \\opreset, \\opmin, \\opprint       17   \n",
       "5         \\opreset, \\opmfe, \\opmax, \\opprint       14   \n",
       "6      \\opreset, \\opnext, \\opfirst, \\opprint       10   \n",
       "7      \\opreset, \\opreset, \\oplast, \\opprint        9   \n",
       "8      \\opreset, \\opprev, \\opfirst, \\opprint        7   \n",
       "9        \\opreset, \\opnext, \\opmax, \\opprint        7   \n",
       "10       \\opreset, \\opmfe, \\oplast, \\opprint        6   \n",
       "11      \\opreset, \\opprev, \\oplast, \\opprint        5   \n",
       "12       \\opreset, \\opprev, \\opmin, \\opprint        4   \n",
       "13    \\opreset, \\opselect, \\oplast, \\opprint        3   \n",
       "14    \\opreset, \\opreset, \\opfirst, \\opcount        2   \n",
       "15    \\opreset, \\opreset, \\opreset, \\opprint        1   \n",
       "16  \\opreset, \\opselect, \\opselect, \\opprint        1   \n",
       "17     \\opreset, \\opreset, \\oplast, \\opcount        1   \n",
       "18      \\opreset, \\opnext, \\oplast, \\opprint        1   \n",
       "19     \\opreset, \\opmfe, \\opselect, \\opprint        1   \n",
       "\n",
       "                                                                                       Triggers  \n",
       "0                      [UNK, date, position, points, name, competition, notes, no, year, venue]  \n",
       "1                              [UNK, rank, total, bronze, gold, silver, nation, name, date, no]  \n",
       "2               [name, UNK, notes, year, nationality, rank, location, date, comments, hometown]  \n",
       "3                          [notes, date, title, UNK, role, genre, year, score, opponent, event]  \n",
       "4                             [year, height, UNK, name, position, floors, notes, jan, jun, may]  \n",
       "5              [opponent, date, result, location, rank, site, attendance, notes, city, listing]  \n",
       "6                              [UNK, name, year, edition, birth, death, men, time, women, type]  \n",
       "7               [date, UNK, distance, location, name, year, winner, japanese, duration, member]  \n",
       "8   [name, notes, intersecting, kilometers, location, athlete, nationality, rank, time, design]  \n",
       "9                                                                              [UNK, ethnicity]  \n",
       "10             [place, season, UNK, date, division, tier, builder, cylinders, notes, withdrawn]  \n",
       "11                  [report, date, average, chassis, driver, race, builder, name, notes, works]  \n",
       "12          [division, level, movements, position, season, current, gauge, notes, wheel, works]  \n",
       "13                           [car, finish, laps, led, rank, retired, start, year, UNK, carries]  \n",
       "14                                                           [UNK, network, owner, programming]  \n",
       "15                                      [candidates, district, first, incumbent, party, result]  \n",
       "16                                                [lifetime, name, nationality, notable, notes]  \n",
       "17                                                         [UNK, english, japanese, type, year]  \n",
       "18                                                                               [UNK, comment]  \n",
       "19                                                    [UNK, length, performer, producer, title]  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=10000\n",
    "K = 10\n",
    "L = 5\n",
    "df_data = []\n",
    "for program, triggers in program_triggers.items():\n",
    "    topk = Counter(triggers).most_common(K)\n",
    "    df_data.append([program, program_counts[program], [w for w, l in topk]])\n",
    "df_data = sorted(df_data, key=operator.itemgetter(1), reverse=True)\n",
    "df = pd.DataFrame(df_data, columns=['Operator sequence', '#tables', 'Triggers'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrl}\n",
      "\\toprule\n",
      "                        Operator sequence &  #tables &                                                                                     Triggers \\\\\n",
      "\\midrule\n",
      "     \\opresetselect, \\opresetselect, \\opmax, \\opprint &      109 &                     [\\unktoken, date, position, points, name, competition, notes, no, year, venue] \\\\\n",
      "      \\opresetselect, \\opprev, \\opmax, \\opprint &       68 &                             [\\unktoken, rank, total, bronze, gold, silver, nation, name, date, no] \\\\\n",
      "   \\opresetselect, \\opresetselect, \\opfirst, \\opprint &       29 &              [name, \\unktoken, notes, year, nationality, rank, location, date, comments, hometown] \\\\\n",
      "     \\opresetselect, \\opgroupbymax, \\opfirst, \\opprint &       25 &                         [notes, date, title, \\unktoken, role, genre, year, score, opponent, event] \\\\\n",
      "     \\opresetselect, \\opresetselect, \\opmin, \\opprint &       17 &                            [year, height, \\unktoken, name, position, floors, notes, jan, jun, may] \\\\\n",
      "       \\opresetselect, \\opgroupbymax, \\opmax, \\opprint &       14 &             [opponent, date, result, location, rank, site, attendance, notes, city, listing] \\\\\n",
      "    \\opresetselect, \\opnext, \\opfirst, \\opprint &       10 &                             [\\unktoken, name, year, edition, birth, death, men, time, women, type] \\\\\n",
      "    \\opresetselect, \\opresetselect, \\oplast, \\opprint &        9 &              [date, \\unktoken, distance, location, name, year, winner, japanese, duration, member] \\\\\n",
      "    \\opresetselect, \\opprev, \\opfirst, \\opprint &        7 &  [name, notes, intersecting, kilometers, location, athlete, nationality, rank, time, design] \\\\\n",
      "      \\opresetselect, \\opnext, \\opmax, \\opprint &        7 &                                                                             [\\unktoken, ethnicity] \\\\\n",
      "      \\opresetselect, \\opgroupbymax, \\oplast, \\opprint &        6 &             [place, season, \\unktoken, date, division, tier, builder, cylinders, notes, withdrawn] \\\\\n",
      "     \\opresetselect, \\opprev, \\oplast, \\opprint &        5 &                  [report, date, average, chassis, driver, race, builder, name, notes, works] \\\\\n",
      "      \\opresetselect, \\opprev, \\opmin, \\opprint &        4 &          [division, level, movements, position, season, current, gauge, notes, wheel, works] \\\\\n",
      "   \\opresetselect, \\opwordmatch, \\oplast, \\opprint &        3 &                           [car, finish, laps, led, rank, retired, start, year, \\unktoken, carries] \\\\\n",
      "   \\opresetselect, \\opresetselect, \\opfirst, \\opcount &        2 &                                                           [\\unktoken, network, owner, programming] \\\\\n",
      "   \\opresetselect, \\opresetselect, \\opresetselect, \\opprint &        1 &                                      [candidates, district, first, incumbent, party, result] \\\\\n",
      " \\opresetselect, \\opwordmatch, \\opwordmatch, \\opprint &        1 &                                                [lifetime, name, nationality, notable, notes] \\\\\n",
      "    \\opresetselect, \\opresetselect, \\oplast, \\opcount &        1 &                                                         [\\unktoken, english, japanese, type, year] \\\\\n",
      "     \\opresetselect, \\opnext, \\oplast, \\opprint &        1 &                                                                               [\\unktoken, comment] \\\\\n",
      "    \\opresetselect, \\opgroupbymax, \\opwordmatch, \\opprint &        1 &                                                    [\\unktoken, length, performer, producer, title] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False,escape=False).replace('UNK','\\\\unktoken').replace('reset','resetselect').replace('mfe','groupbymax').replace('opselect','opwordmatch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on perturbed tables\n",
    "- Perturbed data is arranged such that unperturbed questions appear before perturbed questions. This results in words being added in the same order to the vocab (to effect in same word IDs) as in the unperturbed case.\n",
    "- Since the vocabulary has more words in the perturbed case (due to some words exceeding the min cutoff), special words such as `unk_token` are assigned different IDs. We revert this by swapping word IDs appropriately. The goal being that the word IDs of words in perturbed case should be the same as word IDs of words in the unperturbed case. This is done in `notebook_utils.init_data()` using the argument `preserve_vocab` (default value is `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_train_data, perturbed_dev_data, perturbed_test_data, perturbed_utility, _ = notebook_utils.init_data(PERTURBED_DATA_DIR, preserve_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_correct, perturbed_num_examples, perturbed_correct_dict = evaluate(sess, perturbed_dev_data, perturbed_utility.FLAGS.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_correct/num_dev_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question concatenation attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_PHRASES = [\n",
    "    'in not a lot of words',\n",
    "    'if its all the same',\n",
    "    'in not many words',\n",
    "    'one way or another',\n",
    "    'please answer',\n",
    "    'do you know',\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.2293794186959937)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.11115475255302436)\n",
      "(2546, 2546)\n",
      "--------\n",
      "584 283\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.24234092694422624)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.20777690494893952)\n",
      "(2546, 2546)\n",
      "--------\n",
      "617 529\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.17321288295365278)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.26119402985074625)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.22230950510604872)\n",
      "(2546, 2546)\n",
      "--------\n",
      "665 566\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.35899450117831894)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.34092694422623726)\n",
      "(2546, 2546)\n",
      "--------\n",
      "914 868\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.34642576590730556)\n",
      "(2546, 2546)\n",
      "--------\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3275726630007855)\n",
      "(2546, 2546)\n",
      "--------\n",
      "882 834\n"
     ]
    }
   ],
   "source": [
    "num_correct_list = []\n",
    "concatenation_correct_dicts = []\n",
    "combined_dicts = copy.deepcopy(correct_dict)\n",
    "for phrase in ATTACK_PHRASES:\n",
    "    # prefix\n",
    "    qc_attack_data = copy.deepcopy(unprocessed_dev_data)\n",
    "    for wiki_example in qc_attack_data:\n",
    "        if not correct_dict[wiki_example.question_id]:\n",
    "            continue\n",
    "        wiki_example.question = phrase.strip().split() + wiki_example.question\n",
    "    qc_attack_data = data_utils.complete_wiki_processing(qc_attack_data, utility, train=False)\n",
    "    \n",
    "    prefix_correct, _, prefix_correct_dict = evaluate(sess, qc_attack_data, graph.batch_size, graph, model_step)\n",
    "    \n",
    "    assert(prefix_correct == sum(prefix_correct_dict.values()))\n",
    "    \n",
    "    for k, v in combined_dicts.items():\n",
    "        combined_dicts[k] = v and prefix_correct_dict[k]\n",
    "\n",
    "    # suffix\n",
    "    qc_attack_data = copy.deepcopy(unprocessed_dev_data)\n",
    "    for wiki_example in qc_attack_data:\n",
    "        if not correct_dict[wiki_example.question_id]:\n",
    "            continue\n",
    "        wiki_example.question = wiki_example.question + phrase.strip().split()\n",
    "    qc_attack_data = data_utils.complete_wiki_processing(qc_attack_data, utility, train=False)\n",
    "\n",
    "    suffix_correct, _, suffix_correct_dict = evaluate(sess, qc_attack_data, graph.batch_size, graph, model_step)\n",
    "\n",
    "    assert(suffix_correct == sum(suffix_correct_dict.values()))\n",
    "    \n",
    "    for k, v in combined_dicts.items():\n",
    "        combined_dicts[k] = v and suffix_correct_dict[k]\n",
    "    \n",
    "    print(prefix_correct, suffix_correct)\n",
    "    num_correct_list.append([prefix_correct, suffix_correct])\n",
    "    concatenation_correct_dicts.append([prefix_correct_dict, suffix_correct_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate([np.expand_dims(ATTACK_PHRASES,1), np.array(num_correct_list)/num_dev_examples], axis=1), columns=['Attack phrase', 'Prefix','Suffix'])\n",
    "df.Prefix = df.Prefix.apply(lambda x: str(round(100*float(x), 1)) + '%')\n",
    "df['Suffix'] = df['Suffix'].apply(lambda x: str(round(100*float(x), 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "         Attack phrase & Prefix & Suffix \\\\\n",
      "\\midrule\n",
      " in not a lot of words &  20.6\\% &  10.0\\% \\\\\n",
      "   if its all the same &  21.8\\% &  18.7\\% \\\\\n",
      "     in not many words &  15.6\\% &  11.2\\% \\\\\n",
      "    one way or another &  23.5\\% &  20.0\\% \\\\\n",
      "         please answer &  32.3\\% &  30.7\\% \\\\\n",
      "           do you know &  31.2\\% &  29.5\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False, escape=True, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of all effective attacks: 0.03285058283292123\n",
      "Union of all baseline attacks: 0.2705757682797598\n"
     ]
    }
   ],
   "source": [
    "combined_dicts = copy.deepcopy(correct_dict)\n",
    "for [prefix_results, suffix_results] in concatenation_correct_dicts[:4]:\n",
    "    for k, v in combined_dicts.items():\n",
    "        combined_dicts[k] = v and prefix_results[k] and suffix_results[k]        \n",
    "print(\"Union of all effective attacks:\", sum(combined_dicts.values())/num_dev_examples)\n",
    "\n",
    "combined_dicts = copy.deepcopy(correct_dict)\n",
    "for [prefix_results, suffix_results] in concatenation_correct_dicts[4:]:\n",
    "    for k, v in combined_dicts.items():\n",
    "        combined_dicts[k] = v and prefix_results[k] and suffix_results[k]\n",
    "print(\"Union of all baseline attacks:\", sum(combined_dicts.values())/num_dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2546"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word deletion attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = \"\"\"\n",
    "                show, tell, did,  me, you, your, my, \n",
    "                our, are,  is, at, were, this, on, would, \n",
    "                and,  for, should,  be, do, I, have, had, \n",
    "                the, there, has,  was, we, get, does, a,  \n",
    "                an,  s,  that,  by,  based, in,  of, bring,\n",
    "                with, to, from, whole, been,  want, wanted,\n",
    "                as, than, got, sorted, draw, listed, chart, \n",
    "                only\n",
    "            \"\"\"\n",
    "STOP_WORDS = STOP_WORDS.strip().split(', ')\n",
    "STOP_WORDS = [w.strip() for w in STOP_WORDS]\n",
    "STOP_WORDS = set(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_data = copy.deepcopy(unprocessed_dev_data)\n",
    "for i, wiki_example in enumerate(stop_word_data):\n",
    "    if not correct_dict[wiki_example.question_id]:\n",
    "        continue\n",
    "    stop_word_data[i].question = [w for w in wiki_example.question if w not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_data = data_utils.complete_wiki_processing(stop_word_data, utility, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.3173605655930872)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "num_correct, _, stop_word_correct_dict = evaluate(sess, stop_word_data, graph.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on applying stop words: 0.28541151536559517\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on applying stop words:\", num_correct/num_dev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overstability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "frequent_attributions = []\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        f.readline()\n",
    "        is_correct = f.readline().strip()\n",
    "        row_labels = f.readline().strip().split('\\t')\n",
    "        col_labels = f.readline().strip().split('\\t')\n",
    "    \n",
    "    if is_correct == 'False':\n",
    "        continue\n",
    "    attrs = np.loadtxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    \n",
    "    synonyms = []\n",
    "    program_mask = get_program_mask(col_labels)\n",
    "    for i, stage in enumerate(col_labels):\n",
    "        if not program_mask[i]:\n",
    "            continue\n",
    "        curr_synonyms = [row_labels[k] for k in np.argpartition(attrs[:, i], -K)[-K:]]\n",
    "        for syn in curr_synonyms:\n",
    "            if syn.startswith(utility.unk_token):\n",
    "                syn = utility.unk_token\n",
    "            synonyms.append(syn)\n",
    "    \n",
    "    frequent_attributions.extend(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_attributions = Counter(frequent_attributions)\n",
    "frequent_attributions.pop('tm')\n",
    "frequent_attributions.pop('cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('many', 493),\n",
       " ('tm_token', 477),\n",
       " ('how', 308),\n",
       " ('number', 248),\n",
       " ('after', 175),\n",
       " ('before', 124),\n",
       " ('total', 121),\n",
       " ('or', 103),\n",
       " ('last', 73),\n",
       " ('next', 54)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_attributions.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.0805184603299293)\n",
      "(2546, 2546)\n",
      "--------\n",
      "0 205\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.12293794186959937)\n",
      "(2546, 2546)\n",
      "--------\n",
      "1 313\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.16496465043205027)\n",
      "(2546, 2546)\n",
      "--------\n",
      "2 420\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.16300078554595443)\n",
      "(2546, 2546)\n",
      "--------\n",
      "3 415\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.18224666142969365)\n",
      "(2546, 2546)\n",
      "--------\n",
      "4 464\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.20109976433621368)\n",
      "(2546, 2546)\n",
      "--------\n",
      "6 512\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.22034564021995287)\n",
      "(2546, 2546)\n",
      "--------\n",
      "8 561\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.23644933228593873)\n",
      "(2546, 2546)\n",
      "--------\n",
      "10 602\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2474469756480754)\n",
      "(2546, 2546)\n",
      "--------\n",
      "13 630\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.26394344069128045)\n",
      "(2546, 2546)\n",
      "--------\n",
      "17 672\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2784760408483896)\n",
      "(2546, 2546)\n",
      "--------\n",
      "22 709\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.2875098193244305)\n",
      "(2546, 2546)\n",
      "--------\n",
      "28 732\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.30597014925373134)\n",
      "(2546, 2546)\n",
      "--------\n",
      "37 779\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.31657501963864887)\n",
      "(2546, 2546)\n",
      "--------\n",
      "48 806\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3283582089552239)\n",
      "(2546, 2546)\n",
      "--------\n",
      "63 836\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.34210526315789475)\n",
      "(2546, 2546)\n",
      "--------\n",
      "83 871\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3523173605655931)\n",
      "(2546, 2546)\n",
      "--------\n",
      "108 897\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3570306362922231)\n",
      "(2546, 2546)\n",
      "--------\n",
      "140 909\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.35860172820109976)\n",
      "(2546, 2546)\n",
      "--------\n",
      "182 913\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3617439120188531)\n",
      "(2546, 2546)\n",
      "--------\n",
      "237 921\n",
      "('dev set accuracy   after ', 92500, ' : ', 0.3648860958366064)\n",
      "(2546, 2546)\n",
      "--------\n",
      "308 929\n"
     ]
    }
   ],
   "source": [
    "question_attention_mask_value = -10000.0\n",
    "utility.word_ids['tm_token'] = utility.entry_match_token_id\n",
    "utility.word_ids['cm_token'] = utility.column_match_token_id\n",
    "curve_data = {}\n",
    "for K in np.append(0, np.unique(np.floor(np.geomspace(1, len(Counter(frequent_attributions)), 25)))):\n",
    "    whitelist = set([w for w, _ in frequent_attributions.most_common(int(K))])\n",
    "    \n",
    "    whitelist = set([utility.word_ids[w] for w in whitelist if not w.startswith(utility.unk_token)])\n",
    "    \n",
    "    if len(whitelist) in curve_data:\n",
    "        continue\n",
    "\n",
    "    reduced_vocab_data = copy.deepcopy(dev_data)\n",
    "    for i, wiki_example in enumerate(reduced_vocab_data):\n",
    "        new_question = []\n",
    "        for w in wiki_example.question:\n",
    "            if w in whitelist:\n",
    "                new_question.append(w)\n",
    "        reduced_vocab_data[i].question = [utility.dummy_token_id] * (graph.question_length - len(new_question)) + new_question\n",
    "        reduced_vocab_data[i].question_attention_mask = [question_attention_mask_value] * (graph.question_length - len(new_question)) + [0] * len(new_question)\n",
    "    \n",
    "    num_correct, _, _ = evaluate(sess, reduced_vocab_data, graph.batch_size, graph, model_step)\n",
    "    curve_data[len(whitelist)] = num_correct\n",
    "    print(len(whitelist), num_correct)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(curve_data.keys(), np.divide(list(curve_data.values()), 947))\n",
    "plt.xscale('symlog')\n",
    "plt.xlabel('num. words in vocab')\n",
    "plt.ylabel('relative accuracy')\n",
    "plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
    "plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
